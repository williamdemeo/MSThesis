space of the Markov process. This section and Chapter 3 develop the context in which we formulate
the new ideas of the paper. In the last section of Chapter 3, Section 3.3, we present the familiar
Krylov subspace and explain why this represents our best approximation to a subspace containing
extremal eigenvectors of the transition probability matrix.‘‘ The ﬁrst section of Chapter 4 describes
the Lanczos algorithm for generating an orthonormal basis for the Krylov subspace. As it stands,
this algorithm is useless for an out-of-core problem such as ours since, by deﬁnition of such problems,
it requires too much data movement; all the computing time is spent swapping data between slow
and fast memory (e.g. between the hard disk and cache). Therefore, we discuss alternatives to
Lanczos and demonstrate that the Lanczos coeﬁicients are readily available through simulations of
the Markov chain, which fact allows us to avoid the standard algorithm altogether. Following this
is a chapter describing the Metropolis algorithm used to produce a reversible stochastic matrix.
It is here that we experiment with the procedure described in Section 4.2 and approximate the
extremal eigenvalues of the matrix, without storing any of its vectors. Finally, Chapter 6 concludes

the paper.

‘or, more precisely, a similarity transformation of this matrix.

