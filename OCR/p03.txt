__       p         _   pg     secg  2 _   t       _ _ Golub _d vaay pLsotp_bc5u_t_ os_ee_ ml_pslol
3
the distribution of interest. This is the purpose of boundi_g convergence rates for Markov chains.
Often the M_kov chains encountered in this context satisfy a condition known in the
physics literature as det,iled b0l0nce. Probabilists call chains with this property reversible. This
.m l  means that the chain has the same  robabilit  law w_ether time moves for_ard or bac___d l
.s p,per, we con,,.der the rate at wh,.ch such cha,.ns conve,,e to, ,t,t,_,n,y d,_,t,Nb,t,N,n.2
There _e a number of di_erent methods in common use for bounding convergence rates of
M_kov chains, and a good review of these methods with many references can be found in _8_. More
recently developed methods, employing log_ithmic Sobolev inequalities, _e reviewed in _2I. Most
of the bounds in cornmon use involve the s,b-dominant eigenvalue of the M_kov chain's tr_nsition
pro6ability matri_, and thus require good approximations to such eigenvalues. In many applications,
however, the transition probability matrix is so large that it becomes impossible to store even a
single vector of the matrix in conventional computer memory. These so called o_t-oJ-core problems
are not amen,ble to tr,ditional ei envalue al orithms3 without modi_cation. This  a er develo s
such a modi_cation for the Markov chain eigenvalue problem. In particul_ it develops a method
for approximating the _rst few eigenvalues of a transition probability matrix when we know the
general structure of the underlying Markov chain. The method does not require storage of large
_atrices or vectors.  Instead we need only simulate the M_kov chain, and conduct a st_tistical
analysis of the simulation.
Here is a look at what follows. Section 2.1 contains a review of the relevant M_kov chain
theory. Readers conversa_t in the asymptotic theory of M_kov chains might wish to at least skim
Section 2.l, ifonly to become famili_ with our notation. Section 2.2 describes functions on the state
IThis is _ot a  recise de__itio_ __  _ticu__ tbe cbai_ _ust bave started fVo_ __ts stat__o_ar  d__ r__  __
rigor is postpo_ed u_til Sectio_ 2.l.
2Tbis aad othe, italicized ter_s _e de__ed ,__   t__
3By cctrad,Nt,No,a1 e,Nge,value algo,,_t__stt _e ,efe, to _ bo,ė Found fo, ex_ple ,_
tbe book by De__el _l] for a _ore rece_t discussio_.
the distribution of interest. This is the purpose of bounding convergence rates for Markov chains.

Often the Markov chains encountered in this context satisfy a condition known in the
physics literature as detailed balance. Probabilists call chains with this property reversible. This
simply means that the chain has the same probability law whether time moves forward or backward.‘
In this paper, we consider the rate at which such chains converge to a stationary distribution?

There are a number of different methods in common use for bounding convergence rates of
Markov chains, and a good review of these methods with many references can be found in  More
recently developed methods, employing logarithmic Sobolev inequalities, are reviewed in  Most
of the bounds in common use involve the sub-dominant eigenvalue of the Markov chain’s transition
probability matrix, and thus require good approximations to such eigenvalues. In many applications,
however, the transition probability matrix is so large that it becomes impossible to store even a
single vector of the matrix in conventional computer memory. These so called out-of-core problems
are not amenable to traditional eigenvalue algorithms3 without modiﬁcation. This paper develops
such a modiﬁcation for the Markov chain eigenvalue problem. In particular it develops a method
for approximating the ﬁrst few eigenvalues of a transition probability matrix when we know the
general structure of the underlying Markov chain. The method does not require storage of large
matrices or vectors. Instead we need only simulate the Markov chain, and conduct a statistical
analysis of the simulation.

Here is a look at what follows. Section 2.1 contains a review of the relevant Markov chain
theory. Readers conversant in the asymptotic theory of Markov chains might wish to at least skim

Section 2.1, if only to become familiar with our notation. Section 2.2 describes functions on the state

‘This is not a precise deﬁnition. In particular the chain must have started from its stationary distribution. Full
rigor is postponed until Section 2.1.

2This and other italicized terms are deﬁned in Section 2.1.

“By “traditional eigenvalue algorithms” we refer to those found, for example, in Golub and Van Loan[5]. See also
the book by Demmel [1] for a more recent discussion.

