
\chapter{Convergence to Gibbs measure on the Ising lattice}
%Chapter 5
\label{cha:an-appl-conv}

\section{Physical Motivation}
% Sec 5.1
\label{sec:physical-motivation}
Before considering the details of the Metropolis algorithm, it helps to
understand the setting in which it was developed. To do so we consider a
typical example, the \emph{Ising model}. This is a model of a system consisting
of $n$ ``sites,'' each site taking values in 
%% $\{-1, +1\}$. 
$\{0, 1\}$. 
It might 
help to imagine these sites as equally spaced points on a line or a circle,
though we are not restricted to such cases. 
%% Then our state space $S$ consists of
%% all possible $n$-dimensional permutations of $-1$'s and $+1$'s. 
The state space $S$ consists of
all possible $n$-dimensional permutations of $0$'s and $1$'s. 
%% Thus, $S = \{-1, +1\}^n$ and $|S| = 2^n$. We can think of each
Thus, $S = \{0, 1\}^n$ and $|S| = 2^n$. We can think of each
site as a node existing in one of two possible positions, say ``on'' or
``off.'' Each state $i \in S$ is %represented by 
a unique permutation $\sigma_i$ of of $0$'s and $1$'s. 
Let $\sigma_n(k)$ denote the position %(on or off) 
of the $k$th node when the system is in state $i$. 
An observable $\phi = \phi(\sigma_i)$ on this space is a function of
the permutations $\sigma_i$, $i\in \{1,\dots, 2^n\}$.
%
%
% ---------------- 31.txt ------------------------------------------------------
%
%

The \emph{energy} of the system, when in state $i$, is defined by the Hamiltonian
\[
H(\sigma_i) = - \sum_{\<j k\>} \sigma_i(j)\sigma_i(k)
\]
where the sum runs over all neart neighbor pairs. In statistical mechanics one is often concerned
with the \emph{Gibbs measure} of state $i$, which is defined by
\begin{equation}
\label{eq:5.1}
\pi(\sigma_i)= Z^{-1} \exp\{-\beta H(\sigma_i)\}
\end{equation}
where $Z^{-1}$ is a normalizing constant, sometimes called the \emph{partition function}. 
For the Ising model, we put  
\begin{equation*}
Z = \sum_{i=1}^{2^n}\exp\{-\beta H(\sigma_i)\}
\end{equation*}
so that $\pi$ is a probability measure. 
Now, letting $k$ denote the so called \emph{Boltzman constant}, if a
classical mechanical system is in thermal equilibrium with its surroundings with
absolute temperature $T$, and is in state $i$ with energy $H(\sigma_i)$, then the
probability density in phase-space of the point representing state $i$ is given by
(\ref{eq:5.1}) with $\beta = (kT)^{-1}$. A fundamental result of ergodic theory 
implies that we can also interpret $\pi(\sigma_i)$ as the proportion of time the
system spends in state $i$. If the system is observed at a random time, the
expected value $\bE_\pi\phi$ of any observable $\phi$ is thus 
\begin{equation}
\label{eq:5.2}
\bE_\pi\phi = 
\sum_{i=1}^{2^n}\phi(\sigma_i) \pi(\sigma_i) = 
%% \frac{\sum_{i=1}^{2^n}\phi(\sigma_i) \exp\{-\beta H(\sigma_i)\}}
%% {\sum_{i=1}^{2^n} \exp\{-\beta H(\sigma_i)\}}
Z^{-1}\sum_{i=1}^{2^n}\phi(\sigma_i) \exp\{-\beta H(\sigma_i)\}.
\end{equation}

Perhaps the number of sites $n$ in our Ising model is so large that it is
impractical or impossible to evaluate~(\ref{eq:5.2}).
We might instead consider importance sampling and generate states with
the probability density $\pi$ given in~(\ref{eq:5.1}).
Then $\phi$ is itself an unbiased estimator of~(\ref{eq:5.2}).

If evaluation of~(\ref{eq:5.2}) is difficult, there is no reason to believe that
evaluation of~(\ref{eq:5.1}) is any easier. However, Metropolis and his
collaborators~\cite{Metropolis:1953} contrived a method for producing an ergodic 
%
%
% ------------------ 32.txt --------------------------------------------------
%
%
Markov chain with transition probability matrix K whose elements $k(x,y)$ satisfy
\[
\sum_x \pi(x) k(x,y) = \pi(x)
\]
By 
%Definition 2.1.4 
Definition~\ref{def:2.1.4}
this means that the Markov chain has stationary distribution $\pi$. Since the
chain is ergodic, we know that it will eventually converge to the desired
distribution. So, our main 
problem---the rate at which such a Markov chain will converge---is of primary
concern in applications of the Metropolis algorithm. Moreover, as we will see
below, the Metropolis algorithm ensures that the Markov chain has the desired
stationary distribution by requiring that the chain satisfy the stronger
condition of reversibility, or ``detailed balance.'' Therefore, Markov chains
produced by the Metropolis algorithm satisfy the assumptions of this paper,
and our theory and methods can be used to bound convergence rates of such chains. 

\section{Description of the Metropolis Algorithm}
% Sec 5.2 
\label{sec:descr-metr-algor}
The following explains how the Metropolis Algorithm is carried out. The exposition is
similar to that given in Hammersley and Handscomb's book 
\emph{Monte Carlo Methods} \cite{Hammersley:1964}. We refer the reader desiring
more details to~\cite[Section 9.3]{Hammersley:1964}.

To begin, choose an arbitrary symmetric Markov chain; that is, a Markov chain whose
transition probability matrix P is symmetric. The matrix P is called the 
\emph{proposition matrix}, and its elements
%, $p(i,j)$,
satisfy, for all $i$ and $j$ in $S$,
\begin{equation}
\label{eq:5.4}
p(i,j) \geq 0, \quad p(i,j) = p(j,i), \quad \sum_m p(i,m)=1.
\end{equation}
Using P, we will derive a new transition probability matrix K with elements
$k(i,j)$ satisfying
\begin{equation}
\label{eq:5.5}
\sum_i \pi(i)k(i,j) = \pi(j)
\end{equation}
For $i\neq j$ define
\begin{equation}
\label{eq:5.6}
k(i,j) = 
\begin{cases}
p(i,j)\pi(j)/\pi(i), & \text{ if $\pi(j)/\pi(i) < 1$,}\\
p(i,j), & \text{ if $\pi(j)/\pi(i) \geq 1$.}
\end{cases}
\end{equation}
%
%
% ----------------- 33.txt ----------------------------------------------
%
%
For $i=j$ define
\begin{equation*}
k(i,j) = 
p(i,i) + \sum_{j\in J_i}\bigl(1 - \frac{\pi(j)}{\pi(i)}\bigr)p(i,j), 
\end{equation*}
where $J_i = \{j : j\neq i, \pi(j)/\pi(i) < 1\}$.
Notice that $\pi(i) > 0$ implies $k(i,j) \geq 0$, and
\begin{align}
\label{eq:5.7}
\sum_j k(i,j) 
&= p(i,i) + 
\sum_{j\in J_i}\left\{\bigl(1 - \frac{\pi(j)}{\pi(i)}\bigr)p(i,j)
+ \frac{p(i,j)\pi(j)}{\pi(i)}\right\} 
+ \sum_{j\in J_i^c}p(i,j)
\nonumber\\
&= p(i,i) + 
\sum_{j\in J_i}p(i,j)
+ \sum_{j\in J_i^c}p(i,j).
\end{align}
Equation~(\ref{eq:5.7}) shows that for each $i$ we have $\sum_j k(i,j) = 1$
(the row sums of K are 1), which confirms that K is a stochastic matrix.

Next we check that K satisfies the following \emph{detailed balance} condition:
\begin{equation}
\label{eq:detailed-balance}
\pi(i)k(i,j) = \pi(j)k(j,i).
\end{equation}
Obviously this holds when $i=j$, so assume $i\neq j$.

First suppose $\pi(i) = \pi(j)$.
In this case, the definition~(\ref{eq:5.6}) and symmetry of P imply that
\[
k(i,j) = p(i,j) = p(j,i) = k(j,i).
\]
Therefore, when $\pi(i) = \pi(j)$ the detailed balance
condition~(\ref{eq:detailed-balance}) clearly holds.

Now suppose that $\pi(i) > \pi(j)$. Then the second case in definition (\ref{eq:5.6}) implies
$k(j,i) =p(j,i)$, while the first case gives
\[
k(i,j) = p(i,j)\pi(j)/\pi(i) =  p(j,i)\pi(j)/\pi(i) = k(j,i)\pi(j)/\pi(i),
\]
and we see that 
$\pi(i)k(i,j) = \pi(j)k(j,i)$ holds.
Finally, a symmetric argument shows that if $\pi(j) > \pi(i)$,
then detailed balance is again satisfied.

To complete the justification of the Metropolis algorithm, we note
detailed balance~(\ref{eq:detailed-balance})
implies that $\pi$ is indeed the stationary distribution for a Markov 
chain with transition probability matrix K; that is, (\ref{eq:5.5}) is
satisfied. Indeed, by the detailed balance condition and the fact that
the row sums of K are 1, we have
\begin{equation*}
\sum_i \pi(i)k(i,j)
=\sum_i \pi(j)k(j,i)
=\pi(j)\sum_i k(j,i)
 = \pi(j).
\end{equation*}


We now summarize the tasks performed when implementing the Metropolis algorithm.
First, pick as a proposition matrix any symmetric stochastic matrix and denote
it by P. Begin 
%
%
% ----------- 34.txt --------------------------------------------------------
%
the simulation of the Markov chain in state $i$, and then take one step of the
proposition Markov chain to arrive in state $j$. That is, choose state $j$
according to the probability mass function $p(i, \cdot)$. 
Next, compute $\pi(j)/\pi(i)$. 
If $\pi(j)/\pi(i) \geq 1$, accept $j$ as the new state. If $\pi(j)/\pi(i) < 1$,
then with probability $\pi(j)/\pi(i)$ accept $j$ as the new state; otherwise
(with probability $1- \pi(j)/\pi(i)$), take $i$ as the new state. Performing
each of these tasks produces one step of the Markov chain represented by the 
transition probability matrix K.


\section{The Ising Model}
% Sec. 5.3 
\label{sec:ising-model}
Returning to the Ising model considered in
Section~\ref{sec:physical-motivation}, we consider how the Metropolis 
algorithm is applied to such a model. Again, start the simulated chain in state
$X_0 = i$ and let $X_1 = j$ be a state chosen according to 
$p(i, \cdot)$, where $p(i,j) = p(j,i)$. Next, compute 
\[
\pi(\sigma_j)/\pi(\sigma_i) = \exp\{-\beta[H(\sigma_j) - H(\sigma_i)]\}
\]
Suppose the energy $H(\sigma_j)$ of the new state satisfies 
$H(\sigma_j) \leq H(\sigma_i)$.
Then $\pi(\sigma_j)/\pi(\sigma_i)$ is at least 1 and we move to state $j$.
On the other hand, if
$H(\sigma_j) > H(\sigma_i)$,
then $\pi(\sigma_j)/\pi(\sigma_i)< 1$ and we take
\begin{equation}
\label{eq:5.8}
X_1 = 
\begin{cases}
j & \text{ with probability $e^{-\beta\Delta H}$},\\
i & \text{ with probability $1-e^{-\beta\Delta H}$}.
\end{cases}
\end{equation}
where $\Delta H = H(\sigma_j) - H(\sigma_i)$ is the change in energy from state $j$ to state $i$.

There are many ways to choose a proposition matrix P. The simplest derives from the
so called \emph{Glauber dynamics}, which proceeds as follows: choose one of the
$n$ sites at random with probability $1/n$---let $s$ denote the chosen
site---and change the position of this site; i.e., subtract its value from 1.
%multiply its value by $-1$. 
The resulting state $j$ is the 
%permutation of 
%-1’s and +1’s
% 0's and 1's 
binary string
given by
\begin{equation*}
\sigma_j(k) = 
\begin{cases}
1-\sigma_i(k) & \text{ if $k =s$},\\
\sigma_i(k) & \text{ if $k =s$},
\end{cases}\quad (k = 1, \dots, d).
\end{equation*}
%
%
% -------------- 35.txt ---------------------------------------------------------
%
%
The procedure we employ below involves a slight variation: choose site $s$ at
random uniformly from the $n$ possible sites. However, instead of changing the
value of the chosen site with absolute certainty, as in Glauber dynamics, we
``flip the switch'' with probability $1/2$. 
Therefore, when $k \neq s$
we have $\sigma_j(k) = \sigma_i(k)$, and when $k = s$,
\begin{equation*}
\sigma_j(s) = 
\begin{cases}
1-\sigma_i(s) & \text{ with probability 1/2},\\
\sigma_i(s) & \text{ with probability 1/2}.
\end{cases}
\end{equation*}
The advantage of this procedure is that the proposition matrix, before altering
it with the Metropolis algorithm, is a bit more interesting than that produced
by traditional Glauber dynamics. For, it has 1/2 along the main diagonal and is
thus aperiodic and converges to a stationary distribution. Glauber dynamics has
period 2, so its proposition matrix does not satisfy the hypotheses of our
convergence theorem (Theorem~\ref{thm-2.1.2}). Keep in mind, however, that any
proposition matrix modified by the Metropolis algorithm satisfies our hypotheses.
\section{Computations}
% Sec 5.4
\label{sec:computations}
\subsection{Computer Programs}
Writing a computer program to simulate realizations of the Markov chain described
above (i.e. with transition matrix K) is straight forward, and we do so using
Matlab and the program {\tt ising.m} which appears in the Appendix.

When performing simulations in the present context, there are a few important
aspects to consider. First, our program begins with a ``hot start,'' which means
that the initial state is picked by assigning each site to the value 0 or 1, each
with probability 1/2.\footnote{We cannot simply draw our initial state
from the stationary distribution since the point of this work
is to deal with cases for which samples from the stationary distribution are not
immediately attainable. Furthermore, we are trying to determine how long it will
take for observations from the simulated chain to represent samples from the
stationary distribution.}
%% figure out how long it will take for observations from the simulated chain to represent samples from
%% the stationary distribution. 
%% Perhaps we should instead have picked our initial state
%% from the stationary distribution. On the contrary, this would defeat the
%% whole purpose of our analysis.  
%% Recall that the method has been developed to deal with
%% cases for which 
%
%
% ----------------- 36.txt --------------------------------------------
%
%
%% sampl from the stationary distribution are not quickly attainable. Furthermore, we are trying to
%% figure out how long it will take for observations from the simulated chain to represent samples from
%% the stationary distribution. 

%% sampl from the stationary distribution are not quickly attainable. Furthermore, we are trying to
%% figure out how long it will take for observations from the simulated chain to represent samples from
%% the stationary distribution. 
%% Second, t

The method we have developed depends solely on covariances
of our observable, which we estimate from the data produced by the
simulations. However, such estimates require that the data come from the
stationary distribution. Therefore, we must discard a number of observations
before using the data to estimate covariances. The number of observations 
to discard depends on how long must we wait 
before we can expect the data to represent samples from the stationary distribution.
%% until we have data from the stationary distribution,
%% and we are back where we started — almost.
When employing our procedure, we can discard a
conservative (very large) number of observations and perform our analysis, deriving a bound on
the convergence time. Then, in all future studies, we will know how much data should be discarded
before samples can be assumed to have come from the stationary distribution.

In considering what observable to use on the state space described above, perhaps the
most obvious is simply the number of nodes in the ``on'' position (i.e., the
number of 1's). Recalling that $\sigma_j(k)\in \{0,1\}$ denotes the position of
the $k$th node when the system is in state $j$, this observable is simply the
sum $\phi(j) = \sum_k\sigma_j(k)$. 
%% where X is the indicator function. 
The program {\tt ising.m} simulates the Markov chain, computes the
values of this observable (and its square) and writes these data to a file
called {\tt phi.dat}. 
The program then computes the first few Lanczos coefficients using covariances
produced by Matlab, so that we can check them against the results we get from the
{\tt lanczos.c} program, which we now describe.

The lanczos.c program (listed, along with its dependencies, in the Appendix) implements
the recursive relations described in Section~\ref{sec:lancz-proc-mark} % Sec 4.2
to compute the Lanczos coefficients by the new
method. It accepts input from the user specifying any observable, any number of
iterations, and any 
%
%
% ---------------  37.txt  -----------------------------------------------
%
%
number of desired Lauczos coefiicients. That is, the observable could have come from a simulation
of any reversible Markov chain and not just our special Metropolis chain. The output of this
program is a matrix T containing the maximum number of Lanczos coefficients (up
to the number requested by the user) before an approximate invariant subspace
was reached. The eigenvalues of the nonzero part of this matrix, in accordance
with the theory developed above, should provide a close approximation to the
eigenvalues of the Markov chain's stochastic matrix.

\subsection{Results}
% Sec. 5.4.2 
\begin{table}
\centering
\label{tab:1}
\caption{Estimates of $\lambda_{\max}(\K) = 0.998746$ using $\phi_1$.}
\vskip2mm
\begin{tabular}{r|c|c|c|c}
simulation \# & $\alpha_1$ & $\beta_1$ & $\lambda_{\max}(\T_2)$& $\beta_2$\\
\hline
1& 0.995107& 0.006960& 0.995220& 0.600243\\
2& 0.995099& 0.005891& 0.994863& 0.477393\\
3& 0.994862& 0.009671& 0.994925& N/A\\
4& 0.995321& 0.009319& 0.995654& N/A\\
5& 0.994787& 0.004872& 0.405446& 0.994742
\end{tabular}
\end{table}
Table~\ref{tab:1} shows the results of 5 simulations each of 100,000 iterations (discarding the first
5,000 observations) for the one dimensional Ising lattice with 10 nodes and $\beta= 1$. Displayed is
the estimate of the first Lanczos coefficient $\alpha_1$ (which is the
eigenvalue estimate after one step; i.e., $\lambda_{\max}(\T_1)$), along with its
error bound $\beta_1$. Appearing in the third column is the largest eigenvalue,
$\lambda_0(T_2)$, of the $2 \times 2$ matrix $\T_2$, and $\beta_2$ appears in the
fourth column when it is was computable. When it was not computable, it was
%% probably 
because an approximate invariant subspace was reached 
at the first step, so $\beta_2$ involved expressions which were close to machine
epsilon and thus could not be computed accurately. Table~\ref{tab:2}  shows the same
information when using a second observable, 
$\phi_2 = \phi_1^2$ (i.e., the number of 1's squared).

\begin{table}
\caption{Estimates of $\lambda_{\max}(K) = 0.998746$ using $\phi_2$}
\label{tab:2}
\begin{tabular}{r|c|c|c|c}
simulation \# & $\alpha_1$ & $\beta$ & $\lambda_{\max}(\T_2)$& $\beta_2$\\
\hline
1& 0.994262& 0.006142& 0.994222& N/A\\
2& 0.994333& 0.002974& 0.994332& N/A\\
3& 0.994307& 0.008892& 0.994374& 0.726320\\
4& 0.994436& 0.012884& 0.994685& 0.361404\\
5& 0.993789& 0.006909& 0.993820& 0.788710
\end{tabular}
\end{table}
The true value of the subdominant eigenvalue for this problem is $\lambda_{\max}(\K) = 0.998746$,
and the next largest is $\lambda_2(\K) = 0.993303$. These were computed directly
from the transition matrix 
%
%
% ---------- 38.txt ----------------------------------------------
%
%
using the Matlab program {\tt tpm.m} listed in the Appendix. They should be very accurate, and it
takes the Matlab routine {\tt eig()} just over 20 minutes of cpu time on a Sun
Ultra Sparc to find all the eigenvalues of K.

As montioned in Section~\ref{sec:gener-lancz-algor} (see
Theorem~\ref{thm:4.1.1}), the $\beta$ coefficients provide conservative error 
bounds on the eigenvalue estimates from the previous Lanczos
step. Unfortunately, for this example $\beta$ appears to be too conservative as
it does not even allow us to bound our estimates away from 1. Furthermore, it  
seems that the estimates generated by the method above fall systematically below
the true value of the subdominant eigenvalue. In every simulation, an
approximate invariant subspace (to machine tolerance) was reached after
computing at most 2 pairs of Lanczos coefficients. For $\phi_1$ we see that 
it was reached after computing only one pair on simulations 3 and 4, similarly
for $\phi_2$ on simulations 1 and 2. These observations indicate that the chosen
observable is close to the slowest mode (or 
eigenfunction) of the process. That is, we should observe fast convergence to an approximate
invariant space. However, the space found might contain only the second slowest mode of the
process and, in that case, our eigenvalue estimates would be closer to the third largest eigenvalue,
instead of the second largest.\footnote{Recall from above that we won't
converge to the eigenspace corresponding to the largest eigenvalue 
(which is always 1), unless we choose a nearly constant observable function.}

The matrix T was produced by the program {\tt lanczos.c} and its eigenvalues were computed
by Matlab, both operations taking a few seconds. Granted, comparing a few seconds to the 20
minutes it takes Matlab to compute all the eigenvales is not really fair, 
since calling Matlab's {\tt eig()} routine is not always the
%
%
% --------- 39.txt -----------------------------------------------------
%
%
optimal traditional method for computing only a few eigenvalues of K. However, recall that
the primary motivation for the new method are those examples where the matrix K is so large
that we can't store even a single column vector in fast memory. Once we start
swapping data to and from slow memory, traditional algorithms can become
intractable, and such examples are easy to come by. For instance, if the Ising
model described has 1000 nodes, it produces a $2^{1000}\times 2^{1000}$
transition probability matrix.
%
%
% 40.txt
%
%



