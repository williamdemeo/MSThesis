\chapter{Conclusion}
% Chapter 6
\label{cha:conclusion}
We conclude with some remarks about the special situations in which our
method is useful.
%few aspects of the cases to which the foregoing applies.
We are interested in the subdominant eigenvalue of a large transition probability matrix. Our
method can be used to approximate these eigenvalues when the following conditions are satisﬁed:
\begin{itemize}
\item 
The Markov chain is reversible (satisfies detailed balance).
\item
We can simulate realizations of the chain as well as functions of these
realizations (observables on the state space).
\end{itemize}
When these conditions hold, we can avoid the traditional Lanczos algorithm, and
estimate the values of the Lanczos coeﬂicients via the variational properties of
observables on the state space. 
There are a number of ideas in this paper that could be expanded upon in future
studies.  Perhaps most obvious would be to try to find analogous techniques for
nonreversible (i.e. nonsymmetric) Markov chains. This is a considerable problem
since the Lanczos methods described above are no longer applicable. Perhaps the
\emph{Arnoldi algorithm} (see \cite[Algorithm 6.9]{Demmel:1997}), or the
\emph{nonsymmetric Lanczos algorithm} could also be modified to exploit
special properties of stochastic matrices. %%  (as we have done with the Lanczos
%% algorithm in the symmetric case). 
%
%
% 41.txt
%
%

When applying the Lanczos algorithm, we usually choose a single starting vector (in the
present context, an observable), perform the algorithm for a number of steps, and gather results.
Then we do the same with a new starting vector and compare the results to those obtained with
the first vector. Proceeding in this way for a number of starting vectors, we can provide statistical
evidence that we have, indeed, located the eigenvalues of interest. However, when there are two
observables, $\phi_1$ and $\phi_2$, of particular interest, perhaps we would
benefit from considering both observables simultaneously. Future studies might
develop an algorithm which incorporates both observables, 
again via $\psi_i = \Pi^{1/2}(\phi_i - \bE_{\pi}\phi_i)$. %% In this case, a
The Lanczos algorithm would then be based on the space 
\[
\sK^*(M,\psi_1, \psi_2, p) =  \ran\{\psi_1, \psi_2, \M\psi_1, 
\M\psi_2, \dots, \M^{p-1}\psi_1, \M^{p-1}\psi_2\}
\]
The Lanczos coefficients would then involve not only the autocovariances,
%$\bC_\pi(\phi_i(n), \phi_i(n+m))$
but also the cross-covariances 
$\bC_\pi(\phi_i(n), \phi_j(n+m))$ (deﬁned in (2.6)).
