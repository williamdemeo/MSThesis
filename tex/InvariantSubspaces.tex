\chapter{Invariant and Approximate Invariant Subspaces}

\section{Invariant Subspaces}

\begin{definition}
%% Definition 3.1.1
A subspace $S \subseteq \R^n$ with the property that
\[
x \in S \quad \Longrightarrow \quad \M x \in S
\]
is said to be \emph{invariant} for $\M$.
\end{definition}

Recall that, having chosen observables $\<\Phi\> = (\phi_1, \dots, \phi_p)$, we
constructed the covariance matrix 
$\C(s) = \Psi^t \M \Psi$. 
If the column space of $\Psi$, which we denote by $\ran(\Psi)$, is an invariant
subspace for $\M$, the definition implies $\M\psi_j \in \ran(\Psi)$.
That is, for each $\psi_i$ there exists a vector
$t$ of coeﬂicients such that $\M \psi_j = \sum_{i=1}^p t_i \psi_{ij}$. 
This is true for all $j$ and, putting each vector of
coefficients into a matrix $\T$, we see that 
$\M \Psi = \Psi \T$. Conversely, $\M\Psi = \Psi \T$ implies that $\M\psi_j$ is a
linear combination of columns of $\Psi$, so $\ran(\Psi)$ is invariant. We have
thus proved the following 
% 15.txt
\begin{fact}
%% Fact 3.1.1 
\label{fact:3.1.1}
The subspace $\ran(\Psi)$ is invariant for $\M$ if and only if there exists 
$\T \in \R^{p\times p}$ such that $\M\Psi = \Psi \T$.
\end{fact}

Consequently,
\begin{fact}
%% Fact 3.1.2
\label{fact:3.1.2}
$\lambda(\T) \subseteq \lambda(\M)$.
\end{fact}
\begin{proof}
%Proof of 3.1.2:
  \begin{align*}
\lambda \in \lambda(\T)
& \quad \Longleftrightarrow  \quad
(\exists v \in \R^p) \T v = \lambda v\\
& \quad \Longleftrightarrow  \quad
\Psi \T v = \lambda \Psi v
& \quad \Longleftrightarrow  \quad
\M \Psi v = \lambda \Psi v
& \quad \Longleftrightarrow  \quad
\lambda \in \lambda(M).
  \end{align*}
The second equivalence follows from Fact~\ref{fact:3.1.1}.  %3.1.1.
\end{proof}
%Facts~\ref{fact:3.1.1} and~\ref{fact:3.1.2} are theoretically useful. To see why
To see why Facts~\ref{fact:3.1.1} and~\ref{fact:3.1.2} are theoretically useful,
consider the equation of Fact~\ref{fact:3.1.1}:
\begin{align*}
& \Psi \T = \M \Psi \\
\Longleftrightarrow \quad & \Psi^t\Psi \T = \Psi^t \M \Psi\\
\Longleftrightarrow \quad & \T = (\Psi^t\Psi)^t \Psi^t \M\Psi.
\end{align*}
In this form, we recognize that $\T = \C^{-1}(0)\C(1)$. That is, using only the
covariance of observables on the state space, we can generate a matrix $\T$ with
the property $\lambda(\T) \subseteq \lambda(\M)$. Recalling that 
$\M = \Pi^{1/2}\P\Pi^{-1/2}$, 
we see that $\M$ is similar to our original stochastic matrix $\P$, 
and thus $\lambda(M) = \lambda(P)$.

\section{Approximate Invariant Subspace}
% Sec 3.2
\label{sec:appr-invar-subsp}
%% We qualified the foregoing by stating that the facts are only theoretically
%% useful. This
We noted above that Facts~\ref{fact:3.1.1} and~\ref{fact:3.1.2} are
theoretically useful. Speaking practically now,
%% This is because 
when choosing observables on the state space we may not be sure that
they will satisfy the 
%
%
% -------------16.txt---------------------------------------------------
%
%
primary assumption underlying the two facts. %Facts 3.1.1 and 3.1.2. 
Recall the assumption: $\ran(\Psi)$ is an invariant
subspace for $\M$ where $\Psi = \Pi^{1/2}(\Phi - \<\Phi\>)$. It may well be the
case that there exists $\psi_j\in \ran(\Psi)$ such that 
$\M\psi_j \notin \ran(\Psi)$, thereby violating the assumption. 
Even if we are lacking an invariant subspace, however, for some applications it
is reasonable to expect that observables can be chosen to provide at least
an \emph{approximate invariant subspace}, which is defined as follows:
\begin{definition}
%% Definition 3.2.1
If the columns of $\Psi\in \R^{d\times p}$ are independent and the norm of the
\emph{residual matrix} $\E = \M\Psi - \Psi \T$ is small for some  
$\T \in \R^{p\times p}$, then
$\ran(\Psi)$ defines an \emph{approximate invariant subspace}. 
\end{definition}
To see how an approximate invariant subspace can be useful for approximating
eigenvalues of $\M$, we recall a theorem from Golub and Van Loan~\cite{Golub:1996}.
\begin{theorem}
% Theorem 3.2.1
\label{thm:3.2.1}  
Suppose $\M \in R^{d\times d}$ and $\T \in \R^{p\times p}$ are symmetric and let
$\E = \M\Q - \Q\T$, where $\Q \in \R^{d\times p}$ is orthonormal 
(i.e., $\Q^t\Q = \I$). Then there exist $\mu_1,\dots, \mu_p \in \lambda(\T)$ and
$\lambda_1, \dots, \lambda_p \in \lambda(\M)$ such that
\[
|\mu_k - \lambda_k| \leq \sqrt{2}\|\E\|_2, \quad \text{ for $k = 1, \dots, p$.}
\]
\end{theorem}
If the subspace $\ran(\Q)$ is an approximate invariant subspace, then the definition implies that
there is a choice $\T$ rendering the error $\|\E\|_2$ small, and thus the
eigenvalues of $\T$ provide a good approximation to those of $\M$.

When considering the foregoing ideas, it is apparent that their application
presents new---but hopefully less prohibitive---problems. As these problems
are the focus of the rest of the paper, now is a good time to examine
them. 

First, the preceding theorem assumes a matrix $\Q$ whose columns form an
orthonormal basis for the approximate invariant subspace. For our problem, 
derivation of such a $\Q$ is tricky, and we must prepare for this. 

Next, having an approximate invariant subspace at our disposal merely tells us
that there exists some matrix $\T$ which makes the error 
%
%
% ------------- 17.txt ----------------------------------------------------
%
%
$\|\E\|_2$ small. We must discover the form of such a $\T$. Moreover, it is
natural to seek that $\T$ which minimizes $\|\E\|_2$ for a given approximate
invariant subspace. 

Finally, in order to apply these ideas to realistic eigenvalue problems, we must
find a practical way to generate %% the most appropriate
a good approximate invariant subspace. 

We now address each of these issues in turn.
%the order raised.

Recall the matrix $\Psi = \Pi^{1/2}(\Phi — \<\Phi\>)$. The columns of this
matrix, though independent (by choice of independent observables), are not
necessarily orthonormal. However, consider the polar decomposition 
$\Psi = \Q \Z$, where $\Q^t\Q = \I$ and $\Z^2 = \Psi^t\Psi$ is a symmetric
positive semidefinite matrix.\footnote{Recall that the polar decompostion is
  derived from the singular value decomposition, $\Psi = \U\Sigma \V^t$, by
  letting $\Q = \U \V^t$ and $\Z = \V\Sigma \V^t$.}
%% ‘Recall that the polar decomposition is derived from the SVD, \Psi = UZJV‘, by letting Q = UV‘ and Z = VEV‘.
Note that $\Z = (\Psi^t\Psi)^{1/2}$ is nonsingular, so $\Q$ has the form
\[
\Q = \Psi \Z^{-1} = \Psi(\Psi^t\Psi)^{-1/2},
\]
and it is clear that $\ran(\Q) = \ran(\Psi)$. 
Perhaps $\ran(\Q)$ is a useful approximation to the invariant
subspace for $\M$. If $\ran(Q)$ is not itself invariant, we have the error 
matrix $\E =\M\Q - \Q\T$.  Below we show that the
$\T$ which minimizes $\|\E\|_2$ is $\T = \Q^t\M\Q$. 
This yields the following
\begin{theorem}
%Theorem 3.2.2
\label{thm:3.2.2}  
If $\Psi = \Q\Z$ is the polar decomposition of $\Psi$, then the matrix
\[
\T = \Q^t\M\Q = (\Psi^t\Psi)^{-1/2}\Psi^t \M^t \Psi (\Psi^t\Psi)^{-1/2}
\]
minimizes $\|\E\|_2 = \|\M\Q - \Q\T\|_2$.
\end{theorem}
\begin{proof}
We prove the result by establishing the following
\\[6pt]
\underline{Claim:} Given $\M \in \R^{d\times d}$ suppose $Q \in  \R^{d\times d}$
satisfies $\Q^t\Q =\I$. 
Then,
\[
\min_{\T\in \R^{p\times p}} \|\M\Q - \Q\T\|_2 = \|(\I- \Q\Q^t)\M\Q\|_2,
\]
and $\T = \Q^t\M\Q$ is the minimizer.
%
%
% -------------- 18.txt -------------------------------
%
%
The claim is verified by an easy application of the Pythagorean theorem:
For any $\T \in \R^{p\times p}$ we have
\begin{align}
\label{eq:3.1}
\|\M\Q - \Q\T\|^2_2 &= \|\M\Q - \Q\Q^t\M\Q + \Q\Q^t\M\Q - \Q\T\|^2_2 \nonumber\\
&=\|(\I - \Q\Q^t)\M\Q + \Q(\Q^t\M\Q - \T)\|^2_2 \nonumber \\
&=\|(\I - \Q\Q^t)\M\Q\|^2_2 + \|\Q(\Q^t\M\Q - \T)\|^2_2 \\
&=\|(\I - \Q\Q^t)\M\Q\|^2_2 \nonumber
\end{align}
Equality~(\ref{eq:3.1}) holds since $\I - \Q\Q^t$ projects $\M\Q$ onto the
subspace orthogonal to 
$\ran(Q)$. Thus, the two terms in the expression on the right are orthogonal,
and the Pythagorean theorem yields equality. The concluding inequality
establishes that the minimizing $\T$ is that which annihilates the 
second term in~(\ref{eq:3.1}), that is, $\T = \Q^t\M\Q$.

The second equality in Theorem~\ref{thm:3.2.2} is a consequence of the polar decomposition, in
which $\Q = \Psi \Z^{-1} = \Psi(\Psi^t\Psi)^{-1/2}$. Thus,
$\T = (\Psi^t\Psi)^{-1/2}\Psi^t \M^t \Psi (\Psi^t\Psi)^{-1/2}$
is the minimizer, as claimed.
\end{proof}

\subsection{The Krylov Subspace}
% Sec 3.3
\label{sec:krylov-subspace}
Suppose the columns of a matrix $\Q \in \R^{d\times p}$ give an orthonormal
basis for an approximate invariant subspace. Then, as we have seen, 
\begin{enumerate}
\item 
$\T = \Q^t\M\Q$ minimizes $\|\E\|_2 = \|\M\Q - \Q\T\|_2$ and
\item $\exists \mu_1, \dots, \mu_p\in \lambda(\T)$ and 
$\lambda_1,\dots, \lambda_p \in \lambda(\M)$ such that
\[
\|\mu_k-\lambda_k\| \leq \sqrt{2}\|\E\|_2, \quad \text{ for $k=1,\dots, p$.}
\]
\end{enumerate}
%
%
% ---------------- 19.txt --------------------------------------------
%
%
Given an approximate invariant subspace $\ran(\Q)$ of dimension $p$, these facts
tell us what matrix we should use to approximate $p$ elements of the spectrum of
$\M$. Now all we lack is a description of $\ran(\Q)$. 
That is, we have not specified which approximate invariant subspace would best
suit our objective of approximating the subdominant eigenvalue
$\lambda_{\max}(\M)$. For this purpose the following definition is useful:
\begin{definition}
%% Definition 3.3.1
The \emph{Raleigh quotient} of a symmetric matrix $\M$ and a nonzero vector $x$ is
\[
\rho(x, \M) = \frac{x^t\M x}{x^tx}.
\]
\end{definition}
We will denote Raleigh quotient by $\rho(x)$ when the context makes clear what
matrix is involved. 

To find the approximate invariant subspace most appropriate for our problem, we
choose each dimension successively, providing justification at each step. 
We start with one observable $\phi$ on the state space, and let 
$\psi_1 = \Pi^{1/2}(\phi -\E_\pi\phi)$. That is, $\psi_1$ is a centered (mean
zero) observable whose $i$th coordinate is weighted by $\sqrt{\pi(i)}$. 
Notice that the definition 
%% (which comes directly from the definition of ^tII
%% following equation (2.7)) is such that 1/)1 is a constant vector if 
(which comes from the definition of $\Psi$ following Equation~(\ref{eq:2.7})) 
is such that $\psi_1$ is a constant vector if and only if $\phi$ is oonstant on the
state space, in which case $\psi_1$ is the constant zero function. 
%% This fact
%% provides one reason observables that are constant on the state space are not
%% interesting. 
(Observables that are constant on the state space are not interesting.) 
Second, recall that the row sums of the matrix $\P$ are all one, therefore the
eigenvector corresponding to the eigenvalue $\lambda_0(\P) = 1$ is the constant
vector.  By definition of $\M = \Pi^{1/2}\P\Pi^{-1/2}$, we see that the constant
vector is also the eigenvector corresponding to $\lambda_0$.  This will play an
important role in what follows, as it allows us to focus %primarily 
on the subdominant eigenvalue 
$\lambda_{\max}(\M) = \max\{\lambda_1(\M), |\lambda_{d-1}(\M)|\}$
rather than on $\lambda_0(M)$ (which we already know is 1).

Now, notice that
\begin{equation}
  \label{eq:3.2}
|\rho(\psi_1,\M)| \leq \max |\rho(x,\M)| = \lambda_{\max}(\M)
\end{equation}
% 20.txt
where the max is taken over all nonconstant vectors. Since our interest 
centers on 
$\lambda_{\max}(\M)$, 
we would like a $\psi_1$ that makes the left hand side of~(\ref{eq:3.2}) large. 
This would be achieved if $\psi_1$ were to lie in the space spanned by, say, the
first two eigenvectors of the Markov chain (sometimes referred 
to as the \emph{slowest modes} of the process). However, this subspace spans
only two dimensions of the entire $d$-dimensional space, and it is more likely
that $\psi_1$ only comes close, at best, to lying in the subspace of
interest. Now, given $\psi_1$, a judicious choice for the second dimension $\psi_2$, and
hence $\Psi_2 = [\psi_1\; \psi_2]$, would be that which makes 
$\max_{a\neq 0} |\rho(\Psi_2 a)|$ as large as possible. To establish that 
this is indeed the right objective, note the following:
\begin{align}
\label{eq:3.3}
\max_{a\neq 0}|\rho(\Psi_2a,\M)| &= \max_{a\neq 0} \left|\frac{a^t \Psi^t_2 \M \Psi_2 a}{a^t \Psi^t_2 \Psi_2 a}\right|\nonumber\\
&= \max_{x\in \ran(\Psi_2)}|\rho(x,\M)|\nonumber\\
&= \max|\rho(x,\M)|\\
&= \lambdamax\nonumber
\end{align}
Again, the max on the right side of~(\ref{eq:3.3}) is over nonconstant vectors. 
In other words, we wish to chose
$\Psi_2 = [\psi_1\; \psi_2]$ so that there is a vector $a \in \R^2$ making 
$|\rho(\Psi_2 a,\M)|$ close to $\lambdamax$.

Now, $\rho(\psi_1)$ changes most rapidly in the direction of the gradient
$\nabla \rho(\psi_1)$.
\begin{equation}
  \label{eq:3.4}
\nabla \rho(\psi_1) = \left(\frac{\partial \rho(\psi_1)}{\partial \psi_1(1)}, \dots,
\frac{\partial \rho(\psi_1)}{\partial \psi_1(d)}\right)
 = \frac{2}{\psi_1^t\psi_1} (\M\psi_1 - \rho(\psi_1)\psi_1).
\end{equation}
So, to maximize the left hand side of~(\ref{eq:3.3}), $\Psi_2$ should be chosen
so that the subspace $\ran(\Psi_2)$ contains the gradient vector.
That is, we must choose $\psi_2$ so that
\begin{equation}
\label{eq:3.5}
\nabla \rho(\psi_1) \in \ran\{\psi_1, \psi_2\} = \ran(\Psi_2).
\end{equation}
Clearly, Equation~(\ref{eq:3.4}) implies 
$\nabla \rho(\psi_1) \in \ran\{\psi_1, \M\psi_1\}$.
Thus, if $\ran\{\psi_1, \psi_2\} = \ran\{\psi_1, \M\psi_1\}$, 
then~(\ref{eq:3.5}) is satisfied.
%
%
% ------------ 21.txt ----------------------------------------------
%
%
In general, having chosen $\Psi_k = [\psi_1 \; \dots \; \psi_k]$ so that
$\ran\{\psi_1, \psi_2, \dots, \psi_k\} = \ran\{\psi_1, \M\psi_1, \dots,
\M\psi_k\}$,
we must chose $\psi_{k+1}$ so that for any nonzero vector $a\in \R^k$,
\begin{equation}
\label{eq:3.6}
\nabla \rho(\Psi_ka) \in \ran\{\psi_1, \psi_2, \dots, \psi_k\}.
\end{equation}
Now,
\[
\nabla \rho(\Psi_ka)  = 
 = \frac{2}{a^t\Psi_k^t\Psi_ka} (\M\Psi_k - \rho(\Psi_ka)\Psi_ka).
\]
and therefore,
\[
\nabla \rho(\Psi_ka)  \in 
\ran(\M\Psi_k) \cup \ran(\Psi_k) = \ran\{\psi_1, \M\psi_1, \dots, \M^k\psi_1\}.
\]
Thus, the requirement~(\ref{eq:3.6}) is satisfied when 
\[
\ran\{\psi_1, \psi_2, \dots, \psi_k\} = \ran\{\psi_1, \M\psi_1, \dots,
\M^k\psi_1\}.
\]

In conclusion, the $p$-dimensional approximate invariant subspace that is most suitable
for our objective is
\[
\sK(\M, \psi_1, p) = \ran\{\psi_1, \M\psi_1, \dots, \M^{p-1}\psi_1\}.
\]
This is known as the \emph{Krylov subspace}. Therefore, to answer the problem posed at the outset
of this section, if we take the columns of $\Q$ to be an orthonormal basis for 
$\sK(\M,\psi_1, p)$, then the eigenvalues of $\T = \Q^t\M\Q$ should provide good
estimates of $p$ extremal eigenvalues of $\M$. 
%
%
% --------------- 22.txt ---------------------------------------
%
%
