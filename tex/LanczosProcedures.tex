\chapter{Lanczos Procedures}

The following will review the usual procedure for generating an orthonormal basis for
$\sK(\M, \psi_1, p)$, 
and conclude by showing that, for our problem, we can find the matrix 
$\Q^t\M\Q$
without actually carrying out this procedure. Note that this conclusion is essential since the procedure
requires matrix vector multiplication---an operation we have assumed impossible for large enough $d$.

\section{The General Lanczos Algorithm}
% Sec 4.1
\label{sec:gener-lancz-algor}
Let $\psip{k} =  \M^k \psi$ and consider the $d\times d$ matrix
\[
\Psi = [\psi, \M\psi, \dots, \M^{d-1}\psi] = [\psip{0}, \psip{1}, \dots, \psip{d-1}]
\]
Notice that $\M\Psi = [\psip{1}, \psip{2}, \dots, \psip{d-1}, \M^d \psi]$,
and assuming for the moment that $\Psi$ is nonsingular,
we can compute the vector $h = -\Psi^{-1}\M^d \psi$. Thus,
\begin{equation}
\label{eq:10000}
\M\Psi = \Psi [e_2,\dots,e_d,-h],  
\end{equation}
where $e_j$ is the column vector with 1 in
the $j$th position and zeros elsewhere. We define $H = [e_2,\dots,e_d,-h]$,
so~(\ref{eq:10000}) becomes $\M\Psi = \Psi H$.
%
%
% ----------------- 23.txt ----------------------------------------------
%
%
Equivalently,
\begin{equation}
  \label{eq:4.1}
H = 
\begin{pmatrix}
0 & 0      & \cdots & 0 & -h_1 \\
1 & 0      & \cdots & 0 & -h_2\\
  & \ddots &        &   & \vdots \\
  &        & \ddots & 0 & -h_{d-1}\\
  &        &        & 1 & -h_{d}
\end{pmatrix} = \Psi^{-1} \M \Psi.
\end{equation}
$\H$ is a \emph{companion matrix} which means that its characteristic polynomial
is $p(x) = x^d + \sum_{i=1}^d h_i x^{i-1}$.
Since $\H$ is similar to $\M$, finding the eigenvalues of $\M$ is equivalent to
finding the roots of $p(x)$. However, this is of little practical use since
finding $h$, constructing $p(x)$, and finding its roots is 
probably a harder problem than the one we started with. Instead, the value of decomposition (4.1)
derives from its \emph{upper Hessenberg form}. We exploit this property below.

Let $\Psi = \Q\mR$ be the QR decomposition of $\Psi$. Since $\Psi$ is assumed nonsingular,
\[
\Psi_d^{-1}\M \Psi_d = (\mR^{-1}\Q^t)\M(\Q\mR) = \H
\]
Therefore, $\Q^t\M\Q = \mR\H\mR^{-1}$.  Let $\T \mR\H\mR^{-1}$.  Then, since $\mR$
and $\mR^{-1}$ are both upper triangular and $\H$ is upper Hessenberg $\T = \mR\H\mR^{-1}$ is also upper
Hessenberg. Furthermore, since $\M$ is symmetric, it is clear that 
$\T^t =\Q^t\M\Q = T$. Thus, $\T$ is both upper Hessenberg and symmetric. Therefore,
$\T$ is tridiagonal and we can write it as follows:
\begin{equation}
  \label{eq:4.2}
\T = 
\begin{pmatrix}
\alpha_1 & \beta_1 &      0 & \cdots & 0 \\
\beta_1  & \alpha_2 & \ddots &        & \vdots\\
         & \ddots & \ddots & \ddots   &\\
\vdots   &       &  \ddots & \ddots & \beta_{d-1}\\
0  &   \cdots    &   & \beta_{d-1} &\alpha_d
\end{pmatrix}.
\end{equation}
%
%
% ------------ 24.txt ----------------------------------------------
%
%
Equating columns $j$ on both sides of $\M\Q = \Q\T$ yields
\begin{equation}
\label{eq:4.3}
\M q_j = \beta_{j-1}q_{j-1}+\alpha_j q_j + \beta_j q_{j+1}
\end{equation}
Since the columns of $\Q$ are orthonormal, multiplying both sides 
of~(\ref{eq:4.3})
by $q_j$ and $q_{j+1}$ yields
$\alpha_j = q_j^t \M q_j$ and $\beta_j = q_{j+1}^t \M q_j$.
The foregoing justifies what is called the \emph{Lanczos algorithm}, which is performed as follows:
         \begin{algorithm}
           \caption{Lanczos algorithm for partial reduction to symmetric
             tridiaganal form}
           \begin{algorithmic}
             \STATE $q_1 = \psi/\|\psi\|_2$, $\beta_0 = 0$, $q_0= 0$
             \FOR{$j = 1, \dots, p$}
             \STATE $z \leftarrow \M q_j$
             \STATE $\alpha_j \leftarrow q_j^tz$
             \STATE $z \leftarrow z - \alpha_j q_j - \beta_{j-1} q_{j-1}$
             \STATE $\beta_j \leftarrow \|z\|_2$
             \IF{$\beta_j=0$}
             \STATE return
             \ENDIF
             \STATE $q_{j+1} = z/\beta_j$
             \ENDFOR
           \end{algorithmic}
         \end{algorithm}
The $q_j$ computed by the Lanczos algorithm are often called the \emph{Lanczos vectors}. If the
loop in the algorithm is terminated because $\beta_{k}= 0$, this indicates that
an exact invariant subspace has been computed, and is given by 
$\ran\{q_1, \dots, q_k\}$. Otherwise, we usually halt the algorithm after 
$p$ steps, in which case the algorithm converges to approximations of at most
  $p$ eigenvalues.\footnote{A Detailed discussion of such convergence (and
    misconvergence) is given in~\cite{Demmel:1997}.}

Note that Equation~(\ref{eq:4.3}) results from the equation $\M\Q = \Q\T$. The latter equation holds
since, starting with $d$ vectors in our subspace, $\Q$ is a $d \times d$ matrix
whose columns form an orthonormal basis for all of $\R^d$, which is clearly an
invariant subspace. The Lanczos algorithm above, however, 
%
%
% ------------ 25.txt ---------------------------------------------------------
%
%
proceeds for only $p$ steps, producing a $d \times p$ matrix $\Q$, whose columns
form an orthonormal basis for the \emph{approximate} invariant subspace 
$\sK(\M, \psi, p)$, and a $p \times p$ matrix $\T, = \Q^t\M\Q$. In that case,
$\|\M\Q - \Q\T_p\|_2 = \|\E\|_2$ is nonzero and gives the error bound described
in Theorem~\ref{thm:3.2.1}. Now, writing the full $d \times d$ matrix $\T$
of~(\ref{eq:4.2}) as
\begin{equation}
  \label{eq:4.4}
\T = 
\left( \begin{array}{c|c}
\T_p & \T_{pu}^t \\
 \hline
\T_{pu} & \T_{u}
\end{array}
\right)
=
\left( \begin{array}{cccc|cccc}
\alpha_1 & \beta_1 &      &                 & & && \\
\beta_1 & \ddots  & \ddots&                 & & && \\
        & \ddots  & \ddots  & \beta_{p-1}    &          & && \\
        &         & \beta_{p-1}  & \alpha_{p} & \beta_{p}& && \\
\hline
        &         &             & \beta_{p}  & \alpha_{p+1} & \beta_{p+1} && \\
        &         &             &           & \beta_{p+1}  & \ddots & \ddots &\\
        &         &             &           &             & \ddots & \ddots &\beta_{d-1} \\
        &         &             &           &             &        & \beta_{d-1} &\alpha_{d}
\end{array}
\right)
\end{equation}
allows us to describe the error bound $\|\E\|_2$ in terms of the submatrix
$\T_{pu}$, and hence in terms of $\beta_p$.
\begin{theorem}
%Theorem 4.1.1
\label{thm:4.1.1}
If $\T_p$ and $\T_{pu}$ are the matrices appearing in~(\ref{eq:4.4}), and if the $p$
columns of $\Q$ are computed by the 
Lanczos algorithm, then there exist $\mu_1, \dots, \mu_p \in \lambda(\T_p)$ and 
$\lambda_1, \dots, \lambda_p \in \lambda(\M)$
such that
\[
|\mu_k - \lambda_k| \leq \|\T_{pu}\|_2 = \beta_p, \quad \text{ for $k=1,\dots,p$.}
\]
\end{theorem}
For a proof, see~\cite[Page 365]{Demmel:1997}.
%
%
% --------------- 26.txt -------------------------
%
%
\section{A Lanczos Procedure for Markov Chains}
Notice that the Lanczos algorithm requires the computation of $\M q_j$. Throughout this
paper we have assumed that $\M \in \R^{d\times d}$, and that $d$ is so large as
to make the matrix vector multiplication $\M q_j$ impossible. Now, the Lanczos
algorithm constructs an orthonormal basis for the Krylov subspace. For our
purposes, this construction is not essential. What is essential is that we find
a way to generate the Lanczos coeficients $\alpha_i$, $\beta_j$, $(j = 1,\dots, p)$, and
whence the matrix $\T$, without performing the operation $\M q_j$ required by the
Lanczos algorithm. We now address this problem.

Begin with the centered and weighted observable 
$\psi = \Pi^{1/2} (\phi - \bE_\pi\phi)$,
and let $q_1 = \psi/\|\psi\|_2$.
To simplify notation, let $\phi(n) = \phi(X_n)$. 
The form of the first Lanczos coeﬂicient $\alpha_1$ is straight
forward:
\[
\alpha_1 = q_1^t \M q_1  = \frac{\psi^t\M\psi}{\psi^t\psi}.
\]
and now the special properties of the Markov chain setting play an important
role. Indeed, we have
\begin{align}
\label{eq:4.5}
\psi^t\M\psi &= (\phi - \bE_\pi\phi)^t\Pi^{\frac{1}{2}t}\M\Pi^{\frac{1}{2}}(\phi- \bE_\pi \phi)\nonumber\\
&= \bC_\pi (\phi(n), \phi(n + 1)).
\end{align}
and
\begin{align}
\label{eq:4.6}
\psi^t\psi &= (\phi - \bE_\pi\phi)^t\Pi(\phi- \bE_\pi \phi)\nonumber\\
&= \Var_\pi(\phi(n)).
\end{align}
Recall that the definition of $\M$ implies 
$\Pi^{\frac{1}{2}t}\M\Pi^{\frac{1}{2}}= \Pi \P$, and this justifies 
Equation~(\ref{eq:4.5}). Putting
%
%
% -------------- 27.txt ------------------------------
%
%
these equations together, we have a nice form for $\alpha_1$:
\[
\alpha_1= \frac{\bC_\pi (\phi(n), \phi(n + 1))}{\Var_\pi(\phi(n))}.
\]
The next Lanczos coefficient, $\beta_1$, is only slightly more complicated.
\begin{align}
\label{eq:4.7}
\beta_1 &= \|\M q_1 - \alpha_1 q_1\|_2 \nonumber\\
&= \bigl[(\M q_1 - \alpha_1 q_1)^t(\M q_1 - \alpha_1 q_1)\bigr]^{1/2}\nonumber\\
&= \bigl[q_1^t\M^2 q_1 - \alpha^2_1\bigr]^{1/2}.
\end{align}
Again recalling the definition $\M =\Pi^{1/2}\P\Pi^{1/2}$, a simple manipulation verifies
$\Pi^{\frac{1}{2}t}\M^s\Pi^{\frac{1}{2}} = \Pi \P^s$,
whereby it follows that
\begin{align*}
q_1^t\M^2 q_1  &= \frac{(\phi - \bE_\pi\phi)^t\Pi \P^2(\phi- \bE_\pi \phi)}
{(\phi - \bE_\pi\phi)^t\Pi (\phi- \bE_\pi \phi)} \\
&= \frac{\bC_\pi (\phi(n), \phi(n + 2))}{\Var_\pi(\phi(n))}.
\end{align*}
Inserting the last expression into Equation~(\ref{eq:4.7}) gives
\[
\beta_1 = \left[\frac{\bC_\pi (\phi(n), \phi(n + 2))}{\Var_\pi(\phi(n))}
-\bigl( \frac{\bC_\pi (\phi(n), \phi(n + 1))}{\Var_\pi(\phi(n))}\bigr)^2
\right]^{1/2}.
\]
Notice that we can estimate these expressions of $\alpha_1$ and $\beta_1$ by
running simulations of the Markov chain. 
To do so, we run the Markov chain starting from a random initial state, compute the value of
the observable at each step of the chain and, from the data, compute estimates of the covariances
and variance of the observable.

Before generalizing this for the $j$th Lanczos coefficient, we consider one more coeﬂicient
in detail. Recall that the second basis element $q_2$ in the Lanczos algorithm is given by 
$q_2 = (\M q_1 — \alpha_1 q_1)/\beta_1$. 
From this we can derive an expression for $\alpha_2$ which, while not as neat as those
%
%
% ----------------- 28.txt ----------------------------------------------------------
%
%
for $\alpha_1$ and $\beta_1$, is nonetheless tractable.
\begin{align}
\label{eq:4.8}
\alpha_2 &= q_2^t \M q_2 \nonumber\\
&= \frac{1}{\beta_1^2}(\M q_1 - \alpha_1 q_1)^t\M (\M q_1 - \alpha_1 q_1)\nonumber\\
&= \frac{1}{\beta_1^2}(q_1^t\M^3 q_1 - 2\alpha_1 q_1^t\M^2 q_1 + \alpha_1^2q_1^t\M q_1 )\nonumber\\
&= \frac{1}{\beta_1^2}(q_1^t\M^3 q_1 - 2\alpha_1 \beta_1^2 + \alpha_1^3 )
%&= \bigl[q_1^t\M^2 q_1 - \alpha^2_1\bigr]^{1/2}.
\end{align}
Equation~(\ref{eq:4.8}) follows from Equation~(\ref{eq:4.7}), which can be
written as $q_1^t\M^2 q_1 = \alpha_1^2 + \beta_1^2$.  
The preceding paragraphs define all but one of the quantities in the last
exression of~(\ref{eq:4.8}). The one remaining
is $q_1^t\M^3 q_1$, which can also be expressed in terms of covariances of $\phi$,
and the analysis is almost identical to that for $q_1^t\M^2q_1$:
\begin{align*}
q_1^t\M^3 q_1  &= \frac{(\phi - \bE_\pi\phi)^t\Pi \P^3(\phi- \bE_\pi \phi)}
{(\phi - \bE_\pi\phi)^t\Pi (\phi- \bE_\pi \phi)} \\
&= \frac{\bC_\pi (\phi(n), \phi(n + 3))}{\Var_\pi(\phi(n))}.
\end{align*}
To generalize the foregoing for higher order Lanczos coeﬂicients requires a recurrence
relation between $(\alpha_{j+1}, \beta_j)$ and $(\alpha_{j}, \beta_{j-1})$. 
As we will see, such a relation involves higher order
Lanczos vectors $q_j$---vectors we do not wish to compute. Recall that the only ``vector'' we can
handle is our initial observable. Therefore, we must also establish a recurrence
among the $q_j$'s, thus making them available inductively from our observable vector.

To begin with the $\beta_j$'s, recall that 
$\beta_j = \|\M q_j - \alpha_jq_j - \beta_{j-1}q_{j-1}\|_2$. 
Whence,
\begin{align}
\label{eq:4.9}
\beta_j^2 &= (\M q_j - \alpha_jq_j - \beta_{j-1}q_{j-1})^t(\M q_j - \alpha_jq_j - \beta_{j-1}q_{j-1})\nonumber\\
&= q_j^t\M^2 q_j - \alpha_j^2 - \beta^2_{j-1}.
\end{align}
Some simple algebra and the definitions $\alpha_j = q_j^t\M q_j$ and
$\beta_{j-1} = q_j^t \M q_{j-1}$ prove equality~(\ref{eq:4.9}).  The
%
%
% -------------- 29.txt ------------------------------------------------------
%
%
recurrence for $\alpha_{j+1}$ is equally straight forward, though the final form
is not as neat as the expression above. Indeed,
\begin{align}
\label{eq:4.10}
\alpha_{j+1} &= q_{j+1}^t \M q_{j+1} \nonumber\\
&= \frac{1}{\beta_j^2}(\M q_j - \alpha_j q_j - \beta_{j-1} q_{j-1})^t\M 
  (\M q_j - \alpha_j q_j - \beta_{j-1} q_{j-1})\nonumber\\
&= \frac{1}{\beta_j^2}(q_j^t\M^3 q_j 
    - 2\alpha_j q_j^t\M^2 q_j 
    - 2\beta^2_{j-1} q_{j-1}^t\M^2 q_j 
    + \alpha_1^3
    + 2\alpha_j \beta^2_{j-1} + \beta^2_{j-1}).
\end{align}
From~(\ref{eq:4.9}), $\alpha_j^2 +\beta_j^2 + \beta^2_{j-1}$,
and upon plugging this into~(\ref{eq:4.10}), we arrive at 
\begin{equation}
\label{eq:4.11}
\alpha_{j+1}= \frac{1}{\beta_j^2}(q_j^t\M^3 q_j 
    - \alpha_j^3
    - 2\alpha_j\beta^2_{j}
    - 2\beta_{j-1}q_{j-1}^t\M^2q_j + \beta^2_{j-1}).
\end{equation}

The expressions above involve the higher order Lanczos vectors $q_j$. In the present context,
these vectors will not be readily available. However, another recursion---this time relating forms
involving $q_{j+1}$ to forms involving $q_j$ and $q_{j-1} $---will make these
expressions accessible by induction on our initial observable. The general forms
of interest are $q_{j+1}^t\M^k q_{j+1}$ and $q^t_{j+1}\M^k q_j$. Again, we 
begin with the simpler of the two:
\begin{align*}
  q_{j+1}^tM^kq_j &= 
 \frac{1}{\beta_j}(\M q_j - \alpha_j q_j - \beta_{j-1} q_{j-1})^t\M^k q_j\\
 \frac{1}{\beta_j}(q_j^t\M^{k+1} q_j - \alpha_j q_j^t\M^{k} q_j -\beta_{j-1} q_j^t\M^k).
\end{align*}
The second is,
\begin{align}
\label{eq:4.12}
  q_{j+1}^tM^kq_{j+1} 
&=  \frac{1}{\beta_j^2}(\M q_j - \alpha_j q_j - \beta_{j-1} q_{j-1})^t\M^k 
  (\M q_j - \alpha_j q_j - \beta_{j-1} q_{j-1})\nonumber\\
&=  \frac{1}{\beta_j^2}(q_j^t\M^{k+2} q_j + \alpha^2_jq_j^t\M^{k} q_j 
  + \beta^2_{j-1} q_{j-1}^t\M^k q_{j-1} - 2\alpha_jq_j^t\M^{k+1} q_j \nonumber\\
& \qquad    
    + 2\alpha_j \beta_{j-1}q_j^t\M^k q_{j-1}
    - 2\beta_{j-1}q_j^t \M^{k+1}q_{j-1}).
\end{align}
The point of all this is that each pair of coeﬂicients $(\alpha_{j+1}, \beta_j)$
can be expressed as a function of the covariance between 
$\phi(n)$ and $\phi(n+ m)$, for $m = 0, \dots, 2j+1$. Since the expressions
are based on four recurrence relations, they become quite complicated as $j$ gets large. However,
defining these recursions in a computer program is trivial, and we use this fact
in Section~\label{sec:computations} % Sec 5.4
when applying the foregoing to a particular problem.
%
%
% ----------------- 30.txt ------------------------------------------------------
%
%

