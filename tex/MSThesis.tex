%\documentclass[11pt]{amsart}
\documentclass{au}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% For completion by editorial office:
 \presentedby{\dots}
 \received{\dots}{\dots}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage[colorlinks=true,urlcolor=black,linkcolor=black,citecolor=black]{hyperref}

\usepackage{amsmath,amssymb,url,mathrsfs}
\usepackage{enumerate}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{scalefnt}
\numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{example}[theorem]{Example}
\newtheorem*{remarks}{Remarks}

\newcommand{\todo}[1]{~\\[5pt]{\bf TODO(wjd):} #1\\[5pt]}
%\newcommand{\suchthat}{\ensuremath{:}}    % (old notation)
\newcommand{\suchthat}{\ensuremath{\mid}}  % (new notation)
%\newcommand{\defeq}{\ensuremath{:=}}      % (old notation)
\newcommand{\defeq}{\ensuremath{=}}        % (new notation)
\newcommand{\<}{\ensuremath{\langle}}
\renewcommand{\>}{\ensuremath{\rangle}}
\newcommand{\beps}{\ensuremath{\boldsymbol{\varepsilon}}}
\newcommand{\bN}{\ensuremath{\mathbf{N}}}
\newcommand{\bA}{\ensuremath{\mathbf{A}}}
\newcommand{\bB}{\ensuremath{\mathbf{B}}}
\newcommand{\bBi}{\ensuremath{\mathbf{B}_i}}
\newcommand{\sB}{\ensuremath{\mathcal{B}}}
\newcommand{\sC}{\ensuremath{\mathcal{C}}}
\newcommand{\sS}{\ensuremath{\mathscr{S}}}
\newcommand{\sT}{\ensuremath{\mathscr{T}}}
\newcommand{\sI}{\ensuremath{\mathcal{I}}}
\newcommand{\sE}{\ensuremath{\mathcal{E}}}
\newcommand{\sO}{\ensuremath{\mathcal{O}}}
%\newcommand{\id}{\ensuremath{\mathrm{id}}}
\DeclareMathOperator{\id}{id}
%\newcommand{\Eq}{\ensuremath{\mathrm{Eq}}}
\DeclareMathOperator{\Eq}{Eq}
%\newcommand{\Cg}{\ensuremath{\mathrm{Cg}}}
\DeclareMathOperator{\Cg}{Cg}
%\newcommand{\Con}{\ensuremath{\mathrm{Con\,}}}
\DeclareMathOperator{\Con}{Con}
%\newcommand{\Sub}{\ensuremath{\mathrm{Sub}}}
\DeclareMathOperator{\Sub}{Sub}
%\newcommand{\Pol}{\ensuremath{\mathrm{Pol}}}
\DeclareMathOperator{\Pol}{Pol}
%\newcommand{\Clo}{\ensuremath{\mathrm{Clo}}}
\DeclareMathOperator{\Clo}{Clo}
\newcommand{\ps}[1]{\ensuremath{^{(#1)}}}
\newcommand{\piB}{\ensuremath{\pi_B}}
\newcommand{\hpsi}{\ensuremath{\widehat{\psi}}}
\newcommand{\htheta}{\ensuremath{\widehat{\theta}}}
\newcommand{\supi}{\ensuremath{^{i}}}
\newcommand{\supj}{\ensuremath{^{j}}}
\renewcommand{\leq}{\ensuremath{\leqslant}}
\renewcommand{\nleq}{\ensuremath{\nleqslant}}
\renewcommand{\geq}{\ensuremath{\geqslant}}
\newcommand{\meet}{\ensuremath{\wedge}}
\newcommand{\join}{\ensuremath{\vee}}
\newcommand{\Meet}{\ensuremath{\bigwedge}}
\renewcommand{\Join}{\ensuremath{\bigvee}}
\newcommand{\resB}{\ensuremath{|_{_B}}}
\newcommand{\resBi}{\ensuremath{|_{_{B_i}}}}
\newcommand{\eps}{\ensuremath{\varepsilon}}
\newcommand{\hatmap}{\ensuremath{\widehat{\phantom{x}}}}
\newcommand{\one}{\ensuremath{\mathbf{1}}}
\newcommand{\two}{\ensuremath{\mathbf{2}}}
\newcommand{\three}{\ensuremath{\mathbf{3}}}
\newcommand{\tbeta}{\ensuremath{\widetilde{\beta}}}
\newcommand{\ttheta}{\ensuremath{\widetilde{\theta}}}
\newcommand{\hbeta}{\ensuremath{\widehat{\beta}}}
\newcommand{\cick}{\ensuremath{\sC_i}}
%\newcommand{\CE}{\ensuremath{\sC_r^{\sE}}}
\newcommand{\CE}{\ensuremath{\sB_r^n}}
\newcommand{\CO}{\ensuremath{\sC_r^{\sO}}}
\newcommand{\CICK}{\ensuremath{c_i/\beta \cup c^{K+1}_i/\beta^{\bB_{K+1}}}}
\newcommand{\GAP}{{\small GAP}}
\newcommand{\dotsize}{.8pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  FRONT MATTER                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[Approximating Eigenvalues of Large Stochastic Matrices]{A Lanczos Procedure for Approximating Eigenvalues of Large Stochastic Matrices}

%% First author (Note: The order of the items here is important!)
\author{William DeMeo}
\email{williamdemeo@gmail.com}
\urladdr{http://www.math.sc.edu/~demeow}
\address{Department of Mathematics\\
University of South Carolina\\Columbia 29208\\USA}

\thanks{The author wishes to thank a number of people who contributed to this
  work.  In particular, the primary referee made many insightful recommendations
  which led vast improvements, not only in the overall exposition, but also in
  the specific algebraic constructions, theorem statements, and proofs.
  This article is dedicated to Ralph Freese and Bill Lampe, who declined to be
  named as co-authors despite their substantial contributions to the project.}  

%% Dedication (Optional)
\dedicatory{This article is dedicated to Ralph Freese and Bill
  Lampe.}

%% AMS subject classification; see http://www.ams.org/msc
%% Only one Primary. Possibly several Secondary.
\subjclass[2010]{Primary: 08A30; Secondary: 08A60, 06B10.} %, 06B15.}

%% Keywords and phrases
\keywords{congruence lattice, finite algebra, finite lattice
  representations}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  MAIN MATTER                                                  %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
The rate at which a Markov chain converges to a given probability distribution
has long been an active area of research. Well known bounds on this rate of
convergence involve the subdominant eigenvalue of the chain’s underlying
transition probability matrix. However, many transition probability matrices are
so large that we are unable to store even a vector of the matrix in fast
computer memory. Thus, traditional methods for approximating eigenvalues are
rendered useless.  

In this paper we demonstrate that, if the Markov chain is reversible, and we
understand the structure of the chain, we can derive the coefficients of the
traditional Lanczos algorithm without storing a single vector. We do this by
considering the variational properties of observables on the chain’s state
space. In the process we present the classical theory which relates the
information contained in the Lanczos coefficients to the eigenvalues of the
Markov chain. 
\end{abstract}

\maketitle

%%%%%%%%%%%%%%%%%%%%%
\chapter{Introduction}
The rate at which a Markov chain converges to a given probability distribution has long
been an active area of research. This is not surprising considering this problem’s relevance to the ar-
eas of statistics, statistical mechanics, and computer science. Markov Chain Monte Carlo (MCMC)
algorithms provide important examples. These algorithms come in handy when we encounter a
complicated probability distribution from which we want to draw random samples. In statistical
mechanics, we might wish to estimate the phase average of a function on the state space. Goodman
and Sokal [6] examine Monte Carlo methods in this context. Examples from statistics occur in the
Bayesian paradigm when we are forced to simulate an unwieldy posterior distribution (see, e.g.,
Geman and Geman 

To implement the MCMC algorithm, we invent a Markov chain that converges to the
desired distribution (this is often accomplished using the Metropolis algorithm
described in Chapter 5). Realizations of the chain will eventually represent
samples from this distribution. Sometimes ``eventually'' -- meaning all but
finitely many terms of the chain -- is just not enough. We need more practical
results. In particular, we want to know how many terms of the chain should be 
discarded before we are sampling from a distribution that is close (in total variation distance) to
% 03.txt
the distribution of interest. This is the purpose of bounding convergence rates for Markov chains.

Often the Markov chains encountered in this context satisfy a condition known in the
physics literature as detailed balance. Probabilists call chains with this property reversible. This
simply means that the chain has the same probability law whether time moves
forward or 
backward.\footnote{This is not a precise definition. In particular the chain must
  have started from its stationary distribution. Full rigor is postponed until Section 2.1.}
In this paper, we consider the rate at which such chains converge to a \emph{stationary distribution}.\footnote{This and other italicized terms are defined in Section 2.1.}

There are a number of different methods in common use for bounding convergence rates of
Markov chains, and a good review of these methods with many references can be found in  More
recently developed methods, employing logarithmic Sobolev inequalities, are reviewed in  Most
of the bounds in common use involve the sub—dominant eigenvalue of the Markov chain's transition
probability matrix, and thus require good approximations to such eigenvalues. In many applications,
however, the transition probability matrix is so large that it becomes impossible to store even a
single vector of the matrix in conventional computer memory. These so called out-of-core problems
are not amenable to traditional eigenvalue algorithms\footnote{By ``traditional
  eigenvalue algorithms'' we refer to those found, for example, in Golub and Van
  Loan[5]. See also the book by Demmel [1] for a more recent discussion.}
without modification. This paper develops 
such a modification for the Markov chain eigenvalue problem. In particular it develops a method
for approximating the first few eigenvalues of a transition probability matrix when we know the
general structure of the underlying Markov chain. The method does not require storage of large
matrices or vectors. Instead we need only simulate the Markov chain, and conduct a statistical
analysis of the simulation.

Here is a look at what follows. Section 2.1 contains a review of the relevant Markov chain
theory. Readers conversant in the asymptotic theory of Markov chains might wish to at least skim
Section 2.1, if only to become familiar with our notation. Section 2.2 describes functions on the state
% 04.txt
space of the Markov process. This section and Chapter 3 develop the context in which we formulate
the new ideas of the paper. In the last section of Chapter 3, Section 3.3, we present the familiar
Krylov subspace and explain why this represents our best approximation to a subspace containing
extremal eigenvectors of the transition probability matrix.\footnote{or, more
  precisely, a similarity transformation of this matrix.} The first section of
Chapter 4 describes the \emph{Lanczos algorithm} for generating an orthonormal basis
for the Krylov subspace. As it stands, this algorithm is useless for an
out-of-core problem such as ours since, by definition of such problems, 
it requires too much data movement; all the computing time is spent swapping data between slow
and fast memory (e.g. between the hard disk and cache). Therefore, we discuss alternatives to
Lanczos and demonstrate that the \emph{Lanczos coeficients} are readily available through simulations of
the Markov chain, which fact allows us to avoid the standard algorithm altogether. Following this
is a chapter describing the Metropolis algorithm used to produce a reversible stochastic matrix.
It is here that we experiment with the procedure described in Section 4.2 and approximate the
extremal eigenvalues of the matrix, without storing any of its vectors. Finally, Chapter 6 concludes
the paper.
% 05.txt

\chapter{Markov Chains}

\section{General Theory}

This review of Markov chain theory can be found in any good probability text. The present
discussion is most similar to that of Durrett [3], to which we refer the reader desiring greater detail.

\subsection{The Basic Setup}

Heuristically, a Markov chain is a stochastic process with a lack of memory property. Here
this means that the future of the process, given its past behavior and its present state, depends only
on its present state. This is the probabilistic analogue of a familiar property of classical particle
systems. Given the position and velocities of all particles at time t, the equations of motion can be
completely solved for the future evolution of the system. Thus, information describing the behavior
of the process prior to time t is superﬂuous. To be a bit more precise, if technical, we need the
following definitions.

\bibliographystyle{spmpsci}
\bibliography{wjd}

\end{document}

%%%%%%%%%%%%%%%%%%%%%  END DOCUMENT %%%%%%%%%%%%%%%%%%%%%%%%%


Definition 2.1.1 Let (S, 8) be a measurable space. A sequence X,., n 2 0, of random variables

taking values in S is said to be a Markov chain with respect to the filtration a(Xo, . . . ,X,,) if for
% 06.txt
a.llBES,

p(x,,+1 e B | a(Xo, . . . ,X,.)) = P(X,.+1 e B 1 o(X,,)) (2.1)

Equation (2.1) merely states that, given the present location of X,,, the past is irrelevant for

predicting the location of X,,+1.

Definition 2.1.2 A function p : S x S —> R is said to be a transition probability if:
1. for each :1: E S, A —> p(a:, A) is a probability measure on (S, S).
2. for each A E 8, :1: —> p(z, A) is a measurable function.

We call X,, a Markov chain with transition probabilities 12,, if
P(Xn+l E B l a(Xn)) =Pn(XnaB) (22)

The spaces (S, S) that we encounter below are standard Borel spaces, so the existence of the
transition probabilities follows from the existence of regular conditional probabilities on Borel spaces
— a standard measure theory result (see e.g. [3] page 230).

Suppose we are given an initial probability distribution p on (S, S) and a sequence p,, of

transition probabilities. We can define a consistent set of finite dimensional distributions by

P<X,- e B,-,0 s 1 s n) = / #(d1‘o) / Po($o.d-'51) . .. / Pn-l(97n—1ad$n) (2.3)
B0 B1 B1:
Furthermore, denote our probability space by
(9,?) = (S{0,1,...},8{0,l,...})

We call this sequence space and it is defined more explicitly by

sf°»1=-~-} = {(wo,w1,...) :w.- e S}

S{°*1""} = ¢7(w :40; E A,- E 3)

% 07.txt
The Markov chain that we will study on this space is simply X" (w) = w,., the coordinate maps.
Then, by the Kolmogorov extension theorem, there exists a unique probability measure P,, on
(9,?) so that the X" (w) have finite dimensional distributions (2.3).

If instead of p, we begin with the initial distribution 6,, i.e. point mass at 2:, then we
denote the probability measure by P3. With such measures defined for each 2:, we can in turn

define distributions P,,, given any initial distribution p, by

PM = / u<da.->P.(A>

That the foregoing construction — which, recall, was derived merely from an initial distri-
bution p and a sequence p,, of transition probabilities — satisfies Definition (2.2) of a Markov chain
is not obvious, and a proof can be found in 

To state the converse of the foregoing, if X,, is a Markov chain with transition probabilities
p,, and initial distribution p, then its finite dimensional distributions are given by (2.3). Proof of
this is also found in 

Now that we have put the theory on a firm, if abstract, foundation, we can bring the
discussion down to earth by making the forgoing a little more concrete. First, we specialize our
study of Markov chains by assuming that our chain is temporally homogeneous, which means that
the transition probabilities do not depend on time; i.e. p,,(w,,, B) E p(w,., B). (This is the stochastic
analogue of a conservative system.) Next we assume that our state space S is finite, and suppose
for all states 1', j E S that p(i, j) Z 0, and E, p(z', j) = 1 for all i. In this case, equation (2.2) takes

a more intuitive form:
P(Xn+l =j I X7: = 7:) =P(iaj)

and our transition probabilities become
% 08.txt
If P is a matrix whose (i, j)th element is the transition probability p(i, 1') then P is a stochastic

matriar, that is, a matrix with elements pgj satisfying

Pij 20: zjpij =1: (i7j=1121"‘)d)

We also refer to P as the transition probability matrix.

Without loss of generality, we can further suppose our Markov chain is irreducible. This
means that, for any states 2', 1‘, starting in state i the chain will make a transition to state 1' at
some future time with positive probability. This state of affairs is often described by saying that
all states communicate. We lose no generality with this assumption because any reducible Markov
chain can be factored into irreducible classes of states which can each be studied separately.

The final two conditions we place on the Markov chains considered below will cost us
some generality. Nonetheless, there remain many examples of chains meeting these conditions,
thus making the present study worthwhile. Furthermore, it may be the case that, with a little
more work, we will be able to drop these conditions in future studies. The first condition is that
the chain is aperiodic. If we let I, = {n 2 1 : p"(:c,a:) > 0}, we call a Markov chain aperiodic if,
for any state 2:, the greatest common divisor of I, is 1. The second assumption is that our chain is

reversible. This characterization is understood in terms of the following definition.
Definition 2.1.3 A measure a is called reversible if it satisfies

;4(w)p(I. y) = u(y)p(y. w)

for all :c and y.

We call a Markov chain reversible if its stationary distribution (defined in Section 2.1.2) is reversible.
% 09.txt
\subsection{A Convergence Theorem}

In succeeding arguments, we use some results concerning the asymptotic behavior of

Markov diains. These results require a few more definitions.

Definition 2.1.4 A measure 1r is said to be a stationary measure if

Z«(z)p<z.y> = we) (2.4)

2
Equation 2.4 says P,,(X1 = y) = 1r(y), and by induction that P,,(X,, = y) = 7r(y) for all n 2 1. If 1r
is a probability measure, then we call 1r a stationary distribution. It represents an equilibrium for
the chain in the sense that, if X0 has distribution 1r, then so does X,. for all 11..

When the Markov chain is irreducible and aperiodic, the distribution of the state at time
n converges pointwise to 1r as n —> oo, regardless of the initial state. It is convenient to state
this convergence result in terms of the Markov chain's transition probability matrix P. Before
doing so, we note that irreducibility of a Markov chain is equivalent to irreducibility (in the usual
matrix theory sense) of its transition probability matrix. Furthermore, it turns out that a transition
probability matrix of an aperiodic Markov chain falls into that class of matrices often called acyclic,
but for simplicity we will call such stochastic matrices aperiodic. With this terminology, we can

state the convergence theorem in terms of the transition probability matrix P.

% Theorem 2.1.1
\begin{theorem}
\label{thm-2.1.1}
Suppose P is irreducible, aperiodic, and has stationary distribution 7r. Then as n —> oo,
p"(i,J') -> #0")
\end{theorem}

The symbol p"(i, j) is to be understood as the (ij) element of the nth power of P.
A Markov chain whose transition probability matrix satisfies the hypotheses of
Theorem~\ref{thm-2.1.1} is called ergodic. If we simulate an ergodic chain for
sufficiently many steps, having 
% 10.txt
begun in any initial state, the final state is a sample point from a distribution which is close to 1r.

To make this statement more precise requires that we define “close”.

Definition 2.1.5 Let 1r be a probability measure on S. Then the total variation distance at time

n with initial state I is given by
AA") = l|P"(z, A) - 7r(-4)||Tv = 1333; I P"(I.A) - 7r(A) I

In what follows, we will measure rate of convergence using the function 'r,(e), the first time after

which the total variation distance is always less than 5. That is,
-r,,(e) = min{m : A,(n) 3 E for all n. 2 m}

To consider the connection between convergence rates and eigenvalues, note that an ape-
riodic stochastic matrix P has real eigenvalues 1 = /\o > A1 2 A2 3  Z /\¢_1 2 -1, where
d = |S'] is the dimension of the state space. For an ergodic chain, the rate of convergence to the
stationary distribution 7r is bounded by a function of the subdominant eigenvalue. By subdominant
eigenvalue we mean that eigenvalue which is second-largest in absolute value, and we denote this
Am“ E max{).1,|A¢_1|}. The function bounding the rate of convergence is given by the following

theorem:

% Theorem 2.1.2
\begin{theorem}
\label{thm-2.1.2}
The quantity T1(€) satisfies
1. 13(5) 5 (1 — /\m,_x)“(ln1r(a:)‘1 +1ne‘1)
2. maxxes r,(e) 2 %/\,,,,x(1 — ).,,,,x)‘1ln(2e)’1
\end{theorem}


As this theorem shows, if we have an upper bound on the subdominant eigenvalue, then we have
an upper bound on the function 11(5). In what follows, we will derive an approximation to the
% 11.txt
subdominant eigenvalue and supply error bounds. Together, an approximation and error bounds
for Am“ provide enough information to make Theorem~\ref{thm-2.1.2} useful.

\section{Functions on the State Space}

Recall that X,.(w) = w,. 6 S denotes the state in which the Markov chain exists at time
11. Suppose that 11> = {¢1, . . . , ¢,,} is a collection of p observables, or functions defined on the state
space S. Furthermore, let these observables be real valued, so ¢.~ : S —> R. It is often useful, as we
will see below, to assume that none of the observables is a constant function. Suppose now that
the state space S is finite with at possible states. Then, since an observable is simply a map of the
state space, we can think of each ¢,- as a vector of d real numbers — the d values that it takes on at
the different states.

Now assume the Markov chain is irreducible, and let 1r denote its stationary distribution.
If we start the chain from its stationary distribution - i.e. suppose X0 has distribution 1r - then
X,. is a stationary process. Furthermore, for each i, ¢,(X,.) is a stationary stochastic process with

mean

E.¢.- = Z) vr(x)¢.(z)

165

and autocovariance function

Cir (¢i(Xn)v ¢i (Xn+a))

E7!’  ‘ E1r¢i) (¢i(Xn+a) " E1r¢i)l

Z Pn(Xn = z1X7I+S = y)  — E1r¢i)  _ E1r¢i)

z,yES
(2.5)

which, by the definition of conditional probability, we can write as,

2 P«(X.. = z)P.(X..+.. = y I X. = 1)(¢s($) — Em) (my) ~ Em)
1.1165
% 12.txt
Z 7r($)P1y (MI) - Em.-) (¢:(1I) - E1r¢i)

:c,y€S

Here pgy denotes the element in row :1: and column y of P’, the sth power of the transition probability

matrix. Similarly, we define the cross-covariance between the function ¢,- at time n and q5,- at time

n+sas

E1!’  _ E7r¢:') (¢i(Xn+a) " E1r¢i)l (2-6)

C1r(¢i(Xn)a (#1 (Xn+a))

Z) vr(x)p;,, (¢.-(w) — Em.-) (my) — Em)

z,yES

Now let (<I>) denote the matrix of mean vectors whose jth column is E,,¢,-1, where 1 = (1, . . . , 1)‘,
and let H = diag(1r(w1), . . . , 1r(wd)), be the d X d diagonal matrix with stationary probabilities 1r(w)
on the main diagonal, and zeros elsewhere. Finally, denoting by C(s) the p X p covariance matrix

whose (i,j)th element is C, (qfi,-(X,.),¢_,< (X,.+,)), we have

0(0) = E(‘1’(Xn) - (‘I>))(‘I’(Xn) - (‘PW
= (‘P-(‘1’))‘1'1(<1’-(‘I>))
0(8) = E(<I>(Xn) — (‘I>))(‘1’(Xn+a) - (‘1’))’

(‘P - (‘1’))'1'IP‘(<1> — (<1>))

Below, we will also find it useful to have at our disposal a new matrix that is similar to the
transition probability matrix. We have in mind the matrix M E Ill/2PI'I‘1/2. As is easily verified,
this allows us to write the covariance matrix as

can = <<I>—<<I>>)°n%*M*n%<<I>—<<I>>>

\II‘M’\II (2.7)

where we have defined ‘II E 111/ 2(<I> —  Recall that our main motivation is that, for out-of-core
problems, traditional eigenvalue algorithms are inadequate. With this fact, and the form (2.7) in
% 13.txt
mind, we will consider using the covariance of observables on the state space to implement the
(otherwise useless) Rayleigh-Ritz procedure, which we describe below. This procedure requires
that M be symmetric. As the next fact demonstrates, this required symmetry is the reason for our

interest in the reversibility property of the Markov chain.

Fact 2.2.1 The matrix M is symmetric if and only if the Markov chain is reversible (i.e. iﬂ the

process satisfies the detailed balance condition).

Proof:

M is symmetric

4:, (1-I1/2P1-I-1/2): = H1/2Pn—1/2
<.—> r1-%*P°n%* = H1/2911-1/2
¢> 1>'r1= = up
~:=> (IIP)‘ = l'IP

Element-wise, the final equality implies that 7|‘,'p,'j = 1rjpj.-. According to Definition 2.1.3, this states

that p is a reversible measure.

% 14.txt

\chapter{Invariant and Approximate Invariant Subspaces}

\section{Invariant Subspaces}
Definition 3.1.1 A subspace S Q R" with the property that

1:ES'=Ma:€S

is said to be invariant for M.

Recall that, having chosen observables <I> = ((151, . . . , qfip), we constructed the covariance
matrix C(s) = \II‘M"\II. If the column space of \II, which we denote by ran(\II), is an invariant
subspace for M, the definition implies Mifij E  That is, for each 111,- there exists a vector
t of coeﬂicients such that M1/Jj = f=1t,-1/)5,-. This is true for all j and, putting each vector of
coefficients into a matrix T, we see that M\II = \IIT. Conversely, M\II = IIIT implies that M1/Jj is a

linear combination of columns of \II, so ran(\II) is invariant. We have thus proved the following
% 15.txt
Fact 3.1.1 The subspace ran(\II) is invariant for M if and only if there exists T E R”? such that

M\II = \IIT.

Consequently,

Fact 3.1.2 )\(T) Q ).(M)

Proof of 3.1.2:

z\E)\(T) _=_ 3v€R"3Tv=).v
<=> \IITv = A\IIv
<=> M\II-u = /\\IIv
< /\ E /\(M)

The second ¢> follows from fact 3.1.1.
Facts 3.1.1 and 3.1.2 are theoretically useful. To see why, write the equation of fact 3.1.1

as follows:

\IIT = M\II
\II‘\IIT = \II‘M\II
T = (\1:'~1r)-‘\1:'M\1:

In this form, we recognize that T = C‘1(0)C(1). That is, using only the covariance of observables
on the state space, we can generate a matrix T with the property A(T) (_Z ).(M). Recalling that
M = H1/2Pl'I‘1/2, we see that M is similar to our original stochastic matrix P, and thus ).(M) =

).(P).

\section{Approximate Invariant Subspace}

We qualiﬁed the foregoing by stating that the facts are only theoretically useful. This

is because when choosing observables of the state space we aren't sure that they will satisfy the
% 16.txt
primary assumption underlying facts 3.1.1 and 3.1.2. Recall the assumption: ran(\II) is an invariant
subspace for M, where ‘II = H1/2(<I> —  It may well be the case that there is a 1p; 6 ran(\II) such
that M1/zj ¢ ran(\II), thereby violating the assumption. However, lacking an invariant subspace, we

might hope that our observables at least provide an approximate invariant subspace.

Deﬁnition 3.2.1 If the columns of ‘II E R“? are independent and the residual matrix E E

M\II —— \IIT is small for some T 6 R7”, then ran(\II) deﬁnes an approximate invariant subspace.

To see how an approximate invariant subspace can be useful in approximating eigenvalues of M,
we extract a theorem from Golub and Van Loan 
% Theorem 3.2.1
\begin{theorem}
\label{thm-3.2.1}  
Suppose M 6 Rd” and T 6 RP"? are symmetric and let E E MQ — QT, where Q 6 Rd"? satisﬁes

Q‘Q = I. Then there exist p1,... ,;z,, E A(T) and A1,. . . , A,, E A(M) such that
ll‘k - Akl S \/§|lE"

fork=1,...,p.
\end{theorem}

If the subspace ran(Q) is an approximate invariant subspace, then the deﬁnition implies that
there is a choice T rendering the error IIEII2 small, and thus, the eigenvalues of T provide a good
approximation to those of M.

When considering the preceding ideas, it becomes apparent that their application will
present new — but hopefully less prohibitive — problems. As these problems summarize the rt of
the paper, now is a good time to examine them. First, the preceding theorem assumes a matrix Q
whose columns form an orthonormal basis for the approximate invariant subspace. For our problem,
derivation of such a Q is tricky, and we must prepare for this. Next, having an approximate invariant

subspace at our disposal merely tells us that there exists some matrix T which makes the error
% 17.txt
"EH2 small. We must discover the form of such a T. Moreover, it is natural to seek that T which
minimizes IIEII2 for a given approximate invariant subspace. Finally, in order to apply these ideas
to real examples of our eigenvalue problem, we must ﬁnd a practical way to generate the most
appropriate approximate invariant subspace. We now address each of these issues in the order
raised.

Recall the matrix I1 E III/2(<I> — (<I>)). The columns of this matrix, though independent
(by choice of independent observables), are not necessarily orthonormal. However, consider the
polar decomposition ‘II = QZ where Q‘Q = I and Z2 = \Il‘\II is symmetric positive semideﬁnite.1

Note that Z = (\II‘\II)l/2 is nonsingular, so Q has the form
Q = ~1:z-1 = \II(\II‘\II)“/2

and it is clear that ran(Q) = ran(\II). Perhaps ra.n(Q) is a. useful approximation to the invariant
subspace for M. If ran(Q) is not itself invariant, we have the error E = MQ — QT. It is easy to
show, as we do in the following proof, that the T which minimizes HE“; is T = Q‘MQ. This yields

the following,
Theorem 3.2.2

If \I1 = QZ is the polar decomposition of \II, then the matrix
T = Q‘MQ = (\1:*\1:)-1/’\1:‘M\I:(\Ir‘\I:)-1/2
minimizes IIEII2 = IIMQ - QTII2

Proof:
We prove the result by establishing the following

(Hal: Given M 6 Rd” suppose Q 6 Rd"? satisﬁes Q'Q =1. Then,

min ||MQ — QTII2 = H(1- QQ')MQll2

TERI‘?

‘Recall that the polar decomposition is derived from the SVD, \II = UZJV‘, by letting Q = UV‘ and Z = VEV‘.

% 18.txt
and T = Q’MQ is the minimizer.
To verify the claim, observe the following application of the Pythagorean theorem:

For any T E R.”"” we have

IIMQ — QT||§ = IIMQ - QQ'MQ + QQ'MQ - QT|l§
||(I - QQ’)MQ + Q(Q'MQ - T) H3

||(1 - QQ‘)MQ||§ + |lQ(Q'MQ - T)||§ (3-1)

2 l|(I — QQ')MQ||§

Equality (3.1) holds since I — QQ' projects MQ onto the subspace orthogonal to ran(Q). Thus,
the two terms in the expression on the right are orthogonal, and the Pythagorean theorem yields
equality. The concluding inequality establishes that the minimizing T is that which annihilates the
second term in (3.1); i.e. T = Q‘MQ.

The second equality in Theorem 3.2.2 is a consequence of the polar decomposition, in

which Q = \IIZ“ = \I:(~I:‘\I:)-1/2. Therefore,
T = (x1z'x1:)-1/2x1:‘M~1:(\1:*~1z)—1/2

is the minimizer.

3.3 The Krylov Subspace

Suppose the columns of a matrix Q E R“? provide an orthonormal basis for an approxi-

mate invariant subspace. Then we have seen,
1. T = Q'MQ minimizes ||E||2 = ||MQ — QT||2 and
2. E|p.1,...,p,, E z\(T) and /\1,...,A, 6 »\(M) such that

ll-‘I: - Ml S \/§|lE||2

% 19.txt
19
fork=1,...,p.

Given an approximate invariant subspace ran(Q) of dimension 1), these facts tell us what matrix we
should use to approximate p elements of M’s spectrum. Now all we lack is a. description of ran(Q).
That is, we have not speciﬁed which approximate invariant subspace would best suit our objective

of approximating the subdominant eigenvalue A,,.a2:(M). 'I‘o do so requires the following deﬁnition:

Deﬁnition 3.3.1 The Raleigh quotient of a symmetric matrix M and a nonzero vector 3 is

p(2:, M) E (:z:‘Ma:)/(z‘a:).

We will also denote this by p(z) when the context makes clear what matrix is involved.

To ﬁnd the approximate invariant subspace most appropriate for our problem, we choose
each dimension successively, providing justiﬁcation at each step. Thus, start with one observable
42 on the state space, and let zpl E 111/ 2(¢ — E,,¢). That is, 1/:1 represents a centered (mean zero)
observable whose ith coordinate is weighted by . First notice that the deﬁnition (which comes
directly from the deﬁnition of ‘II following equation (2.7)) is such that 1/)1 is a constant vector if
and only if :15 is oonstant on the state space, in which case 1/); E 0. This fact provides one reason
observables that are constant on the state space are not interesting. Second, recall that the row
sums of the matrix P are all one and, therefore, the eigenvector corresponding to the eigenvalue
Ao(P) = 1 is the constant vector. By deﬁnition of M E 1'11/2Pl'I‘1/2, we see that the constant vector
is also the eigenvector corresponding to A0  This will play an important role in what follows, as
it allows us to focus primarily on the subdominant eigenvalue z\max(M) E max{}q (M), [A44 
rather than on A0  (which we already know is 1).

Now, notice that

lP(¢1,M)l 5 max |P(x,M)l = »\max(M) (3-2)
% 20.txt
where the max is taken over all non-constant vectors. Since our interest centers on An-.3_x(M), we
would like a. 1/11 which makes the left hand side of (3.2) large. This would be achieved if 1/)1 were to
lie in the space spanned by, say, the first two eigenvectors of the Markov chain (sometimes referred
to as the slowest modes of the process). However, this subspace spans only two dimensions of the
entire d-dimensional space, and it is more likely that ¢1, at best, only comes close to lying in the
subspace of interest. Now, given 1/11, a judicious choice for the second dimension l/)2, and hence
W; E [1/)1 1/:2], would be that which makes max,,¢o |p(\II2a)| is as large as possible. To establish that

this is indeed the pertinent objective, note the following:

   

_ a‘\1:5Mx1:2a
I;1;g<Ip(\I'2a,M)l = 13:3: ————a,W§q,2a
= M
mrggf,mIp(z. )1
5 maxlp(z,M)l (3-3)
= ).,,m(M)

Again, the max on the right side of (3.3) is over non-constant vectors. In words, we wish to chose
\II-2 = [1/:1 ¢2] so that there is a vector a 6 R2 making |p(\II2a,M)| close to )«max(M).

Now, p(1/11) changes most rapidly in the direction of the gradient Vp(¢1).

W0  (iiiiiiv---viiiléi)
= ,,;,l (Mam — p<¢.)«m (3.4)

So, to maximize the left hand side of (3.3), ‘I12 should be chosen so that the subspace ra.n(\II2)

contains the gradient vector. That is, we must choose 1/22 so that

VP(¢'1) E F3l1{¢1y¢2} E F-'%!1(‘I’2) (3-5)

Clearly, equation (3.4) implies Vp(1/:1) E ran{z/11,M1,b1}. Therefore, if ran{1/:1,1/)2} = ra.n{¢1,M¢1},

then (3.5) is satisﬁed.
% 21.txt
In general, having chosen \II,, E [1/)1 - -- 1/11,] so that

ran{'pl9¢21' " 1¢k} = ran{¢1vM1[)la ' ‘ ' vMk—11/’k}

we must chose 1/1;.“ so that, for any non-zero vector u 6 R",

VP(‘I’I:a) 6 ra‘n{¢11 ‘21 - ° - 31/Jk-F1} 
Now,
vp<~I~.a) = gmm — P(‘I’kﬂ)‘I’k“)

and therefore,

Vp(‘II;,a.) E ran(M\II;,) U ra.n(\IIk) = ra.n{¢1, M1/)1, . . . , Mktﬁl}

Thus, requirement (3.6) is met when ran{1/:1, 1/:2, . . . , 1/4+1} = ran{1/)1, M1/:1, . . . , Mk:/11}.

In conclusion, the p-dimensional approximate invariant subspace which is most suitable

for our objective is

/C(Mn/mp) 2 ran{«/:1,M¢1,... ,M"“«/21}

This is known as the K rylov subspace. Therefore, in answer to the problem posed at the outset
of this section, if we take the columns of Q to be an orthonormal basis for IC(M,z/)1, p), then the

eigenvalues of T = Q‘MQ should provide good estimates of p extremal eigenvalues of M.

% 22.txt
\chapter{Lanczos Procedures}

The following will review the usual procedure for generating an orthonormal basis for
lC(M, «/11, p), and conclude by showing that, for our problem, we can find the matrix Q‘MQ without
actually carrying out this procedure. Note that this conclusion is essential since the procedure

requires matrix vector multiplication - an operation we have assumed impossible for large enough :1.

4.1 The General Lanczos Algorithm
Let 21;“) E M"¢ and consider the d x :1 matrix
«I 2 [«/»,M«/»,  ,M"“¢] 2 [«b‘°>,«/»“’,...,¢<‘-‘>1

Notice that M\II = [1/:(1),... ,1/;(d'1),Md¢r] and, assuming for the moment that \I1 is non-singular,

we can compute the vector h = —\II‘1M"1/J. Thus,

M\II = \II[e2,. . . ,e,1, —h] E IIIH


% 23.txt
23

Equivalently,
0 0  0 -—h1
1 0  0 —h2
HE   3 ; =\II“M\II (4.1)
0 "hl1—1
1 —h,;

H is a companion matrix which means that its characteristic polynomial is p(:r) = 2:‘ + 23:1 h,-:r"‘1.
Since H is similar to M, ﬁnding the eigenvalues of M is equivalent to ﬁnding the roots of p(a:).
However, this is of little practical use since ﬁnding h, constructing 12(1), and ﬁnding its roots is
probably a harder problem than the one we started with. Instead, the value of decomposition (4.1)
derives from its upper Hessenberg form. We exploit this property below.

Let ‘II = QR be the QR decomposition of \II. Since ‘I! is assumed nonsingular,
~II;‘M~r.: = (R"Q‘)M(QR) = H
Therefore,
Q‘MQ = RHR-1 2 T

Since R and R“1 are both upper triangular and H is upper Hessenberg, T = R.HR'1 is also upper
Hessenberg. Furthermore, since M is symmetric, it is clear that T‘ = Q‘MQ = T. That is, T is

an upper Hessenberg matrix that is symmetric. Therefore, T is tridiagonal and we can write it as

follows:

T =    (4.2)

ﬁd—1

0  174-1 ad


% 24.txt
24

Equating columns j on both sides of MQ = QT yields
M113‘ = B,-—1qj—1 + 0:91 + ﬁjqj+1 (43)

Since the columns of Q are orthonormal, multiplying both sides of (4.3) by q_,- and q,-+1 yields
oz, = q§Mq,- and ﬂj = q;-+1Mq,-.
The foregoing justifies what is called the Lanczos algorithm, performed as follows:

The Lanczos algorithm for (partial) reduction to symmetric tridiaganal form

91 = ¢/||¢||2, [30 = 0, 40 = 0

for j=1top

z=Mq_,-

5:‘ = ||Z||2
if 5, = 0, quit
¢Ij+1 = 2/51

end for

The qj computed by the Lanczos algorithm are often called the Lanczos vectors. If the
loop in the algorithm is terminated because ﬁg = 0, this indicates that an exact invariant subspace
has been computed, and is given by ran{q1 - - - qk}. Otherwise, we usually halt the algorithm after
p steps, in which case the algorithm converg to approximations of at most 12 eigenvalues.‘

Note that equation (4.3) results from the equation MQ = QT. The latter equation holds
since, starting with d vectors in our subspace, Q is a d Xd matrix whose columns form an orthonormal

basis for all of R‘, which is clearly an invariant “sub” space. The Lanczos algorithm above, however,

‘A detailed discussion of such convergence (and misconvergence) is given in 

% 25.txt
25

proceeds for only p steps, producing a d x p matrix Q, whose columns form an orthonormal basis
for the approximate invariant subspace IC(M, ¢, p), and a p X p matrix T, = Q'MQ. In that case,
IIMQ = QTp|[g =  is non-zero and gives the error bound described in Theorem 3.2.1. Now,

writing the full d x :1 matrix T of (4.2) as

01 51
/71

(4.4)

   

allows us to describe the error bound ||E||-2 in terms of the submatrix Tm, and, hence, in terms of

ﬁzz-
Theorem 4.1.1

HT, and T,“ are the matrices appearing in (4.4), and if the p columns of Q are computed by the

Lanczos algorithm, then there exist pl, . . . ,;t,, E /\(T,) and A1, . . . , A, E A(M) such that

ill» - /\I=i S iiTP'4”2 = 512

fork=1,...,p.

For a proof, see page 365 of [1].

% 26.txt
26

4.2 A Lanczos Procedure for Markov Chains

Notice that the Lanczos algorithm requires the computation of Mqj. Throughout this
paper we have assumed that M 6 Rd“, and that d is so large as to make the matrix vector
multiplication Mqj impossible. Now, the Lanczos algorithm constructs an orthonormal basis for
the Krylov subspace. For our purposes, this construction is not essential. What is essential is that
we ﬁnd a way to generate the Lanczos coejﬁcients oz,-, ﬁj, (j = 1, . . . , p), and whence the matrix
T, without performing the operation Mqj required by the Lanczos algorithm. We now address this
problem.

Begin with the centered and weighted observable 1/1 = 111/ 2 (qS—E,,¢), and let q1 = 11;/||1/JH2.
To simplify notation, let ¢(n) E ¢(X,.). The form of the first Lanczos coeﬂicient a1 is straight

forward:

01 = qiMq1

I/1‘M1/I
¢’¢

and now the special properties of the Markov chain setting play an important role. For, regard the

following

¢‘M«/2 = (¢ — E.¢)‘II%‘MII%(¢ - Em)

Cw (d>(n), ¢(" + 1)) (4-5)

and

W1!’ = (<15 - E«¢)'H(¢ - E«¢)

Varn (¢(")) (4-6)

Recall that the deﬁnition of M implies H%‘MI'I% = HP, and this justiﬁes equation (4.5). Putting

% 27.txt
27

these equations together, we have a. nice form for :11:

_ c, (run), an + 1))
"‘ ' Ta:,(¢(n))

The next Lanczos coeﬂicient, B1, is only slightly more complicated.

[31 = ||M91-0t1tI1||2

[(Mq1 — 0l1¢11)i(M¢]1 - 0101)] 1/2

[t1iM2¢I1 — ail 1/2

(4.7)

Again recalling the deﬁnition M E 111/ 2P1'I‘1/2, a. simple manipulation veriﬁes

1'li‘M’l'[% = HP’, whereby it follows that

(«s — E,,¢)‘HP”(¢ — Em)

"‘M2"‘ (¢ — E,¢)*n<¢ — Em)

C: (¢("), ¢(n + 2))
V3-Ur 

Inserting the last expression into equation (4.7) gives

_ c,(¢(n).¢<n+2)) c,(¢<n>,¢(n+1)> ’ "’
"“ jvar,<¢<n)> "(jvar,<¢<n)) ‘)

Notice that we can estimate these expressions of oz; and ﬂl by running simulations of the Markov
chain. To do so, we run the Markov chain starting from a random initial state, compute the value of
the observable at each step of the chain and, from the data, compute estimates of the covariances
and variance of the observable.

Before generalizing this for the jth Lanczos coefficient, we consider one more coeﬂicient
in detail. Recall that the second basis element q; in the Lanczos algorithm is given by 1;; =

(Mql — oz1q1)/ﬁl. From this we can derive an expression for (12 which, though not as neat as those

% 28.txt
28

for oz; and [31, is nonetheless tractable.

I12 ‘IiMq2

1
[7(M91 — 0t1¢I1)tM(M¢I1 - 11191)
1

1
= f,§(qiM3q1 - 2a1qlM2<11 + a’9iM¢11)
1
1
= —2(<1iM3q1 - 2015i ‘ all (4-8)

1.
The last equality follows from equation (4.7), which can be written as q{M2q1 = off’ + (if. The
preceding paragraphs deﬁne all but one of the quantities in the expression (4.8). The one remaining
is q§M3q1, which can also be expressed in terms of covariances of <15, and the analysis is almost

identical to that for q{M2q1:

(¢ — E,¢)'IIP3(¢ — Em)

"lM3"‘ (¢ — E,¢s)‘H(¢ - Em»)

Cu (¢(")a ¢(n + 3))
Varn(¢(n))

To generalize the foregoing for higher order Lanczos coeﬂicients requires a recurrence
relation between (aj+1, ﬂj) and (aj, ﬂ,~_1). As we will see, such a relation involves higher order
Lanczos vectors q,~ — vectors we do not wish to compute. Recall that the only “vector” we can
handle is our initial observable. Therefore, we must also establish a recurrence among the q,-’s, thus
making them available inductively from our observable vector.

To begin with the ﬂfs, recall that ﬁj E ||Mqj — ajq_.,- — ,H_,~_1q,-_;|[2. Whence,

Bf (M411 — °‘j‘Ij - ﬁj-1qj—1)’(MlIj - Otjqj - 51-19141)

q;-M291‘ - 0? - B,z_1 (4-9)

Some simple algebra and the deﬁnitions aj E q§Mqj and ﬁJ~_1 E q;-Mq,--1 prove equality (4.9). The
% 29.txt
recurrence for 0114.1 is equally straight forward, though the ﬁnal form is not as neat:

2
0tj+1 ¢Ij+1M'1j+1

1
§(M¢15 - 11111;‘ - ﬂj—1<Ij—1)'M(Mqj - Otjqj - /3j—1qj—1)
J

1
§(q;-M-“*4; — 2a,~q;M*q,- — 2x3;+’_1q;--1M*q,- + «I? + 2a,-5,51 + am (4.10)
J
From (4.9), q;~M2q,~ = oz? + ﬂf + ﬂ}_1 and, upon plugging this into (4.10), we arrive at
1
a,~+1 = E(q§»M3q_,,- — (1? — 2ozJ~ﬁJ2 — 2ﬂj_1q;»_1M2qj + ﬁ2_1) (4.11)
J

The expressions above involve the higher order Lanczos vectors qj. In the present context,
these vectors will not be readily available. However, another recursion — this time relating forms
involving q,-+1 to forms involving qj and qj__1 — will make these expressions accessible by induction
on our initial observable. The general forms of interest are q;+1M"qj+1 and qj-+1M"qJ~. Again, we

begin with the simpler of the two:

¢I;'+1Mk9j = ij(Mqj - Otjqj - ﬂj—1t1j—1)'M"¢1j
= i(<1§M"“¢Ij - C‘j‘I_;'Mk‘Ij - ﬂj—1q§M"qj-1)
The second is,
‘I_;'+1Mkqj+1 = ﬂi?(M<Ij - Otjqj - .3j—1<Ij—1)‘Mk(Mqj — Orjqj - ﬂj—1¢1j—1)

1
ﬁ(‘1§M'°"<1:‘ + °‘:2"I§M'°<J:' + ﬁ?—z<Ii—1M"<I:‘—1
J
—2a,-q;-M*+‘q,~ + 2am,--1q;-M*q,--1 — zza,--1q;-M*+‘q]--1) (4.12)

The point of all this is that each pair of coeﬂicients (oz,-+1, /3,-) can be expressed as a
function of the covariance between ¢(n) and ¢(n+ m), for m = 0, . . . , 21' + 1. Since the expressions
are based on four recurrence relations, they become quite complicated as 1' gets large. However,
deﬁning these recursions in a. computer program is trivial, and we use this fact in Section 5.4 when

applying the foregoing to a particular problem.

% 30.txt
30

Chapter 5

An Application: Convergence to

Gibbs Measure on the Ising Lattice

5.1 Physical Motivation

Before considering the details of the Metropolis algorithm, it helps to understand the
setting in which it was developed. To do so we consider an typical example, the Ising model.
Consider a model of a system consisting of 71 “sites”, each site taking values in {-1, +1}. It might
help to imagine these sites as equally spaced points on a line or a circle, though we are not restricted
to such cases. Then our state space 5' consists of all possible n—dimensional permutations of -1’s
and +1’s. We reprent this as S’ = {-1, +1}", and note that |S| = 2". We can think ofeach site as
a node existing in one of two possible positions, say “on" or “off”. Each state 1' E S is represented
by a unique permutation a.- of —1’s and +1’s. Let a,-(k) = :|:l denote the position (on or off) of
the kth node when the system is in state 1'. An observable ¢ = qS(a,-) on this space is a function of

the permutations 47,-, 1' 6 {1,. . . , 2"}.

% 31.txt
31

The energy of the system, when in state i, is deﬁned by the Hamiltonian

H07-‘) = - Z Ua(i)0s(k)

<j Ic>
where the sum runs over all neart neighbor pairs. In statistical mechanics we are often concerned

with the Gibbs measure of state i, deﬁned by
1r(a,-) = Z'1 exp{—[3H(a,~)} (5.1)
where Z ‘1 is the normalization constant, or partition function. For the Ising model, we put

Z = §:9XP{~ﬁH(<7:‘)}
i=1

so that 7r is a probability measure. Now, letting 1: denote the so called Boltzman constant, if a
classical mechanical system is in thermal equilibrium with its surroundings with absolute temper-
ature T, and is in state i with energy H (05), then the probability density in phase-space of the
point representing state i is given by (5.1) with [3 = (lcT)“. The central result of ergodic theory
implies that we can also interpret 1r(o;) as the proportion of time the system spends in state i. If

the system is observed at a random time, the expectation E,,¢ of any observable ¢ is thus

.31 ¢(0a) 9"l>{-ﬂH(0s‘)}

E” = 2?;.exp{—nH(a.-)}

(5.2)

Perhaps the number of sites n in our Ising model is so large that it is impractical or impossible to
evaluate (5.2). We might instead consider importance sampling and generate states with probability

density

exp{—aH<a.o}
2?;.exp{—zaH<a.->} (53)

Then «I: is itself an unbiased estimator of (5.2).

1r(0a') =

If evaluation of (5.2) is difficult, there is no reason to believe that evaluation of (5.3) is any

easier. However, Metropolis and his collaborators [9] contrived a method for producing an ergodic

% 32.txt
32

Markov chain with transition probability matrix K whose elements Ic(:::, y) satisfy

2«<z>k<x.y) = «<2»

I

By Deﬁnition 2.1.4 this means that the Markov chain has stationary distribution 7r. Since the
drain is ergodic, we know that it will eventually converge to the desired distribution. So, our main
problem — the rate at which such a Markov chain will converge — is clearly at issue in the application
of the Metropolis algorithm. Moreover, as we will see below, the Metropolis algorithm ensures that
the Markov chain has the desired stationary distribution by requiring that the chain satisfy the
stronger condition of reversibility; i.e. detailed balance. Therefore, Markov chains produced by the
Metropolis algorithm satisfy the assumptions of this paper, which fact conﬁrms that our theory

can be used to bound convergence rates of all such chains.

5.2 Description of the Metropolis Algorithm

The following explains how the Metropolis Algorithm is carried out. The exposition is
similar to that given in Hammersley and Handscomb’s book Monte Carlo Methods 
To begin, choose an arbitrary symmetric Markov chain; that is, a Markov chain whose

transition probability matrix P is symmetric. P is called the proposition matrix, and its elements,
P071"), Satisfy
pa, 1) 2 0. pm‘) = pm). 2,-pm‘) = 1 vi e S (5-4)

Using P, we will derive a new transition probability matrix K which satisﬁes

Z vr(z')k(='.:') = arm (5.5)

I

For 1' aé j deﬁne

kw) = p<-yum")/«<¢> it nu)/1r(='> <1 (5.6)

P(i,J') if 7r(J')/7r(i) Z1

% 33.txt
33

For 2' = j deﬁne

kw) = pm) (1—  p(-3;"), where J. = {j = 2‘ 7‘ an nu)/«(='> < 1}

Notice that 1r(i) > 0 implies k(i,j) _>_ 0 and

;k<«',j> = pow)  { (1 —  pm) +  +]§§p<«",:')
= P(is'5) + 2 P(i1.1') + Z P(iy.7') (5-7)
jel.» jug

Equation (5.7) shows that, for each i, 2, k(z', j) = 1, which fact conﬁrms that K is a stochastic

matrix.

Next we check that K satisﬁes the detailed balance condition. First suppose that 1r(z') =

1r(j). Then (5.4), (5.6), and symmetry of P imply
k(i,J') = p(i2.7') = p(jvi) = k(j1i)

and, since 1r(i) = 1r(j), the detailed balance condition, 1r(i)k(i,j) = 7r(j)k(i,j), clearly holds. Now

suppose that 1r(i) > 1r(j). Then (5.4) and (5.6) imply k(j,i) =p(j, 1') and
k(i,J') = P(iyJ')7r(J')/7r(i) = P(J3i)7r(J')/7r(i) = k(.1'» i)7r(J')/1r(i)

That is, 1r(i)lc(i,j) = 1r(j)k(i,j), as we set out to prove.
To complete the justification of the Metropolis algorithm, we note that the detailed balance
condition, 7r(i)k(i, j) = 1r(j)k(i, j), implies that 1r is indeed the stationary distribution for a Markov

chain with transition probability matrix K. To see this notice that

27r(i)k(i..7') = Zﬂijlkiiajl = 1r(J') Z/€(i,J‘) = 7r(J')

I i
We now summarize the tasks performed when implementing the Metropolis algorithm.

First, pick as a proposition matrix any symmetric stochastic matrix and denote it by P. Begin

% 34.txt
34

the simulation of the Markov chain in state i, and then take one step of the proposition Markov
chain to arrive in state j. That is, choose j according to the mass function p(z', -). Next, compute
1r(j)/1r(i). If 7r(j)/1r(i) 2 1, accept j as the new state. If1r(j)/1r(i') < 1, then with probability
1r(j)/7r(z') accept j as the new state; otherwise, with probability 1 — 1r(j)/1r(i), take i as the new
state. Performing each of these tasks produces one step of the Markov chain represented by the

transition probability matrix K.

5.3 The Ising Model

Returning to the Ising model considered in section 5.1, we consider how the Metropolis
algorithm is applied to such a mode]. Again, start the simulated chain in state X0 = 1' and let

X1 = j be a state chosen according to p(i, -), where p(i, j) = p(j,i). Next, compute

7"(‘7j)/7"(‘7i) = eXP{-ﬁlH(<7j) - H(0i)l}

Suppose the energy H (Uj) of the new state satisﬁes H (U,-) 3 H (05). Then 1r(a_,-)/1r(a,-) Z 1 and we

move to state j. On the other hand, if H(aj) > H(a;), then 1r(aj)/7r(a,-) < 1 and we take

j with probability e-M"

X1 (5.8)

i with probability 1 — e-W’
where AH E H (0,) — H (:75) is the change in energy from state j to state 1'.
There are many ways to choose a proposition matrix P. The simplest derives from the
so called Glauber dynamics, which proceeds as follows: choose one of the 11. sites at random with
probability 1/n — let .9 denote the chosen site — and change the position of this site; i.e., multiply

its value by -1. The resulting state j is the permutation of -1’s and +1’s given by

-0,-(k) if]: = .9
a,~(Ic)= (k=1,...,d)

a,-(Ic) if]: 95 s

% 35.txt
35

The procedure we employ below involves a slight variation: choose site .9 at random uniformly
from the 11 possible sites. However, instead of changing the value of the chosen site with absolute
certainty, as in Glauber dynamics, we “ﬂip the switch” with probability 1/2. Therefore, when k yé .9

we have 0',-(k) = a.~(k), and when k = 3,

—a,~(s) with probability 1/2
01(3) =

a,-(3) with probability 1/2
The advantage of this procedure is that the proposition matrix, before altering it with the Metropo-
lis algorithm, is a bit more interesting than that produced by traditional Glauber dynamics. For,
it has 1/2 along the main diagonal and, thus, is aperiodic and converges to a stationary distribu-
tion. Glauber dynamics has period 2, so its proposition matrix does not satisfy the hypotheses of

our convergence theorem (Theorem 2.1.2). Keep in mind, however, that any proposition matrix

modiﬁed by the Metropolis algorithm satisﬁes all of our hypotheses.

5.4 Computations

5.4.1 Computer Programs

Constructing a computer program to simulate realizations of the Markov chain described
above (i.e. with transition matrix K) is straight forward, and we do so using Matlab and the
program ising.m which appears in the Appendix.

When performing simulations in the present context, there are a few important aspects
to consider. First, our program begins with a “hot start", which means that the initial state is
picked by assigning each site to :l:1 with probability 1/2. Perhaps we should instead have picked
our initial state from the stationary distribution. On the contrary, this would defeat the whole

purpose of our analysis. Recall that the method has been developed to deal with cases for which

% 36.txt
36

sampl from the stationary distribution are not quickly attainable. Furthermore, we are trying to
ﬁgure out how long it will take for observations from the simulated chain to represent samples from
the stationary distribution. Second, the method we have developed depends solely on covariances
of our observable which we estimate from the data. produced by the simulations. However, such
estimates require that the data come from the stationary distribution. Therefore, we must discard a
number of observations before using the data to estimate covariances. The number of observations
to discard depends on how long must we wait until we have data from the stationary distribution,
and we are back where we started — almost. When employing our procedure, we can discard a
conservative (very large) number of observations and perform our analysis, deriving a bound on
the convergence time. Then, in all future studies, we will know how much data should be discarded
before samples come (approximately) from the stationary distribution.

In considering what observable to use on the state space described above, perhaps the
most obvious is simply the number of nodes in the on position (i.e. the number of +l’s). Recalling
that 0, (k) = i1 denotes the position (on or off) of the kth node when the system is in state j, this

observable is deﬁned by,
«M = zxca.-ac) = +1). 1 = 1.---.ISl
I:

where X is the indicator function. The program ising.m simulates the Markov chain, computes the
values of this observable (as well as the number of 1’s squared) and writes it to a ﬁle called phi . dat.
The program then proceeds to compute the ﬁrst few Lanczos coefficients using covariances produced
by Matlab, so that we can check them against the rults we get from the 1anczos.c program,
which we now describe.

The lanczos .c program (listed in the Appendix along with its dependencies) implements
the recursive relations described in section 4.2 to compute the Lanczos coefﬁcients by the new

method. It accepts input from the user specifying any observable, any number of iterations, and any

% 37.txt
37

TABLE 1: ESTIMATES or /\,m(K) = 0.998746 USING ¢1

  
   
  
  
  
 

     
 

        
  
  

    
   
    
 

      
   
   

   

sim. # /32
1 0.995107 0.006960 0.995220 0.600243
2 0.995099 0.005891 0.994863 0.477393
3 0.994862 0.009671 0.994925 N/ A
4 0.995321 0.009319 0.995654 N/A
5 0.994787 0.004872 0.405446 0.994742

number of desired Lauczos coeﬁicients. That is, the observable could have come from a simulation
of any reversible Markov chain and not just our special Metropolis chain. The output of this
program is a T matrix containing the maximum number of Lauczos coeﬂicients (up to the number
requested by the user) before an approximate invariant subspace was reached. The eigenvalues of
the nonzero part of this matrix, in accordance with the foregoing theory, should well approximate

eigenvalues of the Markov chain’s stochastic matrix.

5.4.2 Results

Table 1 shows the results of 5 simulations each of 100,000 iterations (discarding the ﬁrst
5,000 observations) for the one dimensional Ising lattice with 10 nodes and [7 = 1. Displayed is
the estimate of the ﬁrst Lauczos coeﬂicient a1 (which is the eigenvalue estimate after one step; i.e.
).m,x(T1)), along with its error bound ﬁl. Appearing in the third column is the largest eigenvalue of
the 2 x 2 matrix T2, A0 (T2), and ﬁg appears in the fourth column when it is was computable. When
it was not computable, it was probably because an approximate invariant subspace was reached
at the ﬁrst step, so ﬁg involved expressions which were close to machine epsilon and, thus, could
not be computed accurately. Table 2 shows the same information when using a second observable,
¢2 E ¢§ (i.e. the number of +1’s squared).

The true value of the subdominant eigenvalue for this problem is /\max(K) = 0.998746,

and the next largest is A2(K) = 0.993303. These were computed directly from the transition matrix

% 38.txt
38

TABLE 2: ESTIMATES OF >.,,,,,.,,,(K) = 0.998746 USING z/>2

  
   
  
  
   
  

  
  
   
   

   
  
  
   
  

    
   
   
   

sim. # B2
1 0.994262 0.006142 0.994222 N / A
2 0.994333 0.002974 0.994332 N / A
3 0.994307 0.008892 0.994374 0.726320
4 0.994436 0.012884 0.994685 0.361404
5 0.993789 0.006909 0.993820 0.788710

using the Matlab program tpm.m listed in the Appendix. They should be very accurate, and it
takes the Matlab routine eig() just over 20 minutes of cpu time on a Sun Ultra to ﬁnd all the
eigenvalues of K.

As shown by Theorem 4.1.1, the [Ts provide conservative error bounds on the eigenvalue
estimates from the previous Lanczos step. Unfortunately, for this example, ,6 appears to be too
conservative as it does not even allow us to bound our estimates away from 1. Furthermore, it
seems that the estimates generated by the method above fall systematically below the true value of
the subdominant eigenvalue. In every simulation, an approximate invariant subspace (to machine
tolerance) was reached after computing at most 2 pairs of Lanczos coefficients. For ¢1 we see that
it was reached after computing only one pair on simulations 3 and 4, similarly for ¢2 on simulations
1 and 2. These observations indicate that the chosen observable is close to the slowest mode (or
eigenfunction) of the process. That is, we should observe fast convergence on an approximate
invariant space. However, the space found might contain only the second slowest mode of the
process and, in that case, our eigenvalue estimates would be closer to the third largest eigenvalue,
instead of the second largest as desired.‘

The matrix T was produced by the program lanczos . c and its eigenvalues were computed
by Matlab, both operations taking a few seconds. Granted, comparing a few seconds to the 20

minutes it takes Matlab is not justiﬁed since a simple call to the routine eig() is far from the

 

‘Recall that we explained above why we will never converge to the eigenspace corresponding to the largest eigen-
value (which is always 1). That would require a (nearly) constant observable function.

% 39.txt
39

most eﬂicient traditional method for computing only a few eigenvalues of K. However, recall that
the primary motivation for the new method are those examples where the matrix K is so large
that we can’t store even a single vector in fast memory. Once we start swapping data to and from
cheap memory, the time required by traditional algorithms can become years, even lifetimes. Such
examples are not exceptional. Consider, for instance, the 1000 node problem which produces a.

2“’°° x 21°°° transition matrix.

% 40.txt
40

Chapter 6

Conclusion

We conclude by noting a few aspects of the special cases to which the foregoing applies.
We are interested in the subdominant eigenvalue of a large transition probability matrix. The

foregoing can be used to approximate these eigenvalues when the following conditions are satisﬁed:
o The Markov chain is reversible (detailed balance condition).

0 We can easily simulate realizations of the chain as well as functions (observables) of these

realizations.

When these conditions hold, we can side step the traditional Lanczos algorithm, and get at the
values of the Lanczos coeﬂicients via the variational properties of observables on the state space.
There are many ideas in this paper that should be expanded upon in future studies. Per-
haps most obvious is the desire to ﬁnd analogous techniques for non-reversible (i.e. non-symmetric)
Markov chains. This is a considerable problem since the Lanczos methods described above are no
longer applicable. Perhaps the Amoldi algorithm ([1], algorithm 6.9), or the non-symmetric Lanczos

algorithm can also be usefully specialized for stochastic matrices.

% 41.txt
41

When applying the Lanczos algorithm, we usually choose a single starting vector (in the
present context, an observable), perform the algorithm for a number of steps, and gather results.
Then we do the same with a new starting vector and compare the results to those obtained with
the ﬁrst vector. Proceeding in this way for a number of starting vectors, we can provide statistical
evidence that we have, indeed, located the eigenvalues of interest. However, when there are two
observables 451, :15; of particular interest, perhaps we would beneﬁt by considering both observables
simultaneously. Future studies might develop an algorithm which incorporates both observables,

again via ¢.- = H1/2(¢.' - E,.¢.-). In this case, a Lanczos algorithm would be based on the space

Ic*(M,¢1,¢»2.p) 2 ran{«/»mz»2,M«/»1,M«/:2, . . .,MP—‘¢1.M"—‘«/22}

The Lanczos coeﬁicients would then involve not only the autocovariances C, (¢,<(n),¢,-(n + m)),

but also the cross-covariances C, (¢.~(n), ¢j(n + m)) (deﬁned in (2.6)).

% 42.txt
Appendix: computer programs

Z Hatlab code ising.m

Z
Z Written by William J. De Meo on 12/15/97

2 last modified 1/10/98

Z Inputs:

2 d = number of nodes of the ising lattice (e.g. d=100)
Z n = number of iterations (e.g. n=1000)

Z beta = Annealing schedule (e.g. beta = 2,

1 when beta -> 0 will always go to nee state
Z vhen beta -> infty will never go to state of higher energy)
Z

X = zeros(d,1);
Energy = zeros(n,1);

Ave=0;

X Initialize using hot start
for i=1:d,

if rand < .5

x(i) = -1;

else

X(i) = 1;

end
end

Z Initialize at state of high energy (not used)

if 0
X(1:2:d-1)=1;
X(2:2:d)=—1;
and

Z Initialize energy

H=0;

for i=1:(d-1)

H = H - X(i)*X(i+1);
end

2 Initialize observable
phi = zeros(n+1,1);

for i=1:d
if X(i) == 1
phi(1)= phi(1)+1;
end

end

42

% 43.txt
43

for j=1:n

site=0;
while site==

site = round(rand#d);
end
Z Compute new energy (with new X(site) = —x(site)):
if site ==

Hnew = H + 2:x(1)~x(2);
elseif site == d

Hnew = H + 2*X(d)tX(d-1);
else

Hnew = H + 2*X(site)*(X(site-1) + X(site+1));
end

Z Change to new state in two ways:
X with probability 1/2, flip the switch
if rand < .5
if (Hnew <= H) I (rand < exp(-beta*(Hnew-H)))
x(site) = -x(site);
H = Hnew;
Z Compute new value of observable
if X(site) -= 1
phi(j+1) = phi(j) + 1;

else
phi(j+1) = phi(j) — 1;
end
else phi(j+1)=phi(j);
end
else phi(j+1)=phi(j);
end
Energy(j) = H;
end
p1ot(Energy)

% A second observable
phi2 = zeros(n+1,1);
for i=1:n+1

phi2(i) = phi(i)*phi(i);
end

Z Write first observable to phi.dat:

fid = fopen(’/quean.a/masters/deme5025/research/nyuthesis/programs/c/phi.dat’,’w’);
fprintf(fid,’%f\n’,phi);
fc1ose(fid);

Z Write second observable to phi2.dat:
fid = fopen(’/quean.a/masters/deme5025/research/nyuthesis/programs/c/phi2.dat’,’w’);

% 44.txt
fprintf(fid,’%f\n’,phi2);
fc1ose(fid);

1 Compute a few Lanczos coefficients by new method
START=1001;

V1=zeros(2);

C1=zeros(2);

C2=zeros(2);

C3=zeros(2);

V1 = cov(phi(START:n+1),phi(START:n+1));

V1 = V1(1,1)5

C1 = cov(phi(START:n),phi(START+1:n+1));
C1 = C1(1,2);

C2 = cov(phi(START:n-1),phi(START+2:n+1));
C2 = C2(1,2);

C3 = cov(phi(START:n-2),phi(START+3:n+1));

C3 = C3(1,2);

a1pha1 = C1/V1;

betai = sqrt(C2/V1 ' (C1/V1)‘2);

a1pha2 = (1/(beta1’2))*(C3/V1 - 2*a1pha1*(C2/V1) + a1pha1‘3);

T1 = Ealphal betal; betal a1pha2]

v1 = cov(phi2(START:n+1),phi2(START:n+1));
v1 = v1(1,1);

c1 = cov(phi2(START:n),phi2(START+1:n+1));
c1 = c1(1,2);

c2 = cov(phi2(START:n—1),phi2(START+2:n+1));
c2 = c2(1,2);

c3 = cov(phi2(START:n-2),phi2(START+3:n+1));
c3 = c3(1,2);

alphal = C1/V1;
betal = sqrt(C2/V1 - (C1/V1)‘2);
a1pha2 = (1/(beta1‘2))#(C3/V1 - 2ta1pha1*(C2/V1) + a1pha1”3);

T2 = [a1pha1 betai; betal a1pha2]

fid = fopen(’Tmats.dat’,’v’);
fprintf(fid,’%f\n',T1);
fprintf(fid,’%f\n’,T2);
fc1ose(fid);

ZZZ end ising.m

44
% 45.txt
\% Matlab code tpm.m for generating transition probability matrix
Z and computing its eigenvalues directly

1

Z Written by Hilliam J. De Heo on 1/10/98

1

Z Inputs:

Z d = number of nodes of the ising lattice (e.g. d=10)

Z beta = Annealing schedule (e.g. beta - 2,

1 when beta -> 0 will always go to new state
1 when beta -> infty will never go to state of higher energy)
2

disp(’bui1ding proposition matrix...’)

states = 2‘d;

A = 0;

for i=0:(d-1)

E=eye(2”i);

A = [A E;E A];

end

B = .5#eye(states);

A = B + .5-(1/d)*A;

2 our modified Glauber dynamics requires the B and the .5¥(1/d)

disp(’...done’)

1 display pattern of nonzero entries
1 spy(A)

Z check that all row sums are 1
check=O;
for i=1:states

check = check + (not(sum(A(i,1:states))<.99));
end
1 if not all rows sum to 1, print check = (# of rows with sum=1)
if not(check==states)

check

error(’Row sums are not all 1’)
end

Z construct matrix of states
E = zeros(d,states);
f1ip=-1;
for i=1:d
for j=1:2‘(i-1):states
flip = -1*f1ip;
for k=0:2“(i-1)

45

% 46.txt
E(i,j+k) = flip;
and
end
end
E=E’;

Z display first 64 states
Z E(1:64,:)

Z compute energy of each state
H = zeros(states,1);
for i=1:states

for j=1:d-1
H(i) = H(i) - E(i,j)tE(i,j+1);
end
end

disp(’building Hetropolis transition matrix...')

for i=1:states
for j=i+1:states
if not(A(i,j)==0)
if(H(j)>H(i))
alt = A(i,j)t(1-exp(-beta*(H(j)-H(i))));
A(i,i) I A(i,i)+a1t;
A(i,j) = A(i,j)-alt;
elseif(H(j)<H(i))
alt = A(j,i)t(1-exp(—beta#(H(i)-H(j))));
A(j,j) = A(j,j)+a1t:
A(j,i) = A(j,i)-alt;
end
end
end
end

disp(’...done’)

check=0;
for i=1:states

check = check + (not(sum(A(i,1:states))<.99));
end

Z if not all rows sum to 1, print check = (# of rows with aum=1)

if not(check==states)

check

error(’Row sums are not all 1’)
end

46

% 47.txt
47

disp(’computing eigenvalues of tpm...’)
cput = cputime;

evals = eig(A);

ecput = tputime - cput;

disp(’the CPU time (in secs) for computing eigenvalues of tpm: ’)

ecput
ZZZ end tpm.m

/¥#*##¥###*##*#l####¥#‘###$ﬂ###ﬁi##*U3*#**#*##**##¥***ll¥#*##¥ii

* 1anczos.c main program for computing Lanczos coefficients *
3 #
* Created by William J. De Heo #
t on 1/7/98 *
# Last modified 1/10/98 #

*¥#######**3It¥*#¥¥¥#**##**¥##¥#####**####*#*###*##**###¥##*##*/

#inc1ude <stdlib.h>
#include <stdio.h>
#include <math.h>
#inc1ude "prototypes.h"

/# Machine constants on dino (Sun Ultra 1 at Courant) t/
#define HACHEPS 1.15828708535533604981e-16

#define SQRTEPS 1.07623746699106147048e-08

ﬂdefine HAX_NAH 100

void read_name(char *);

/* Prototypes from the file functions.c ¥/

double a1pha(int j);

double betasq(int j);

double form1(int j, int k);

double form2(int j, int k);

double moment(doub1e ‘data, long n, double tave, double tvar, double tcov, long k);

/# external variables to be used by functions */
double *phi,tcov, tvar, tave;

% 48.txt
main()

{

char tfilename;

FILE *ofp;

double temp=O, *T;

long i, j, nrou, nlanc, START;
int f1ag=O;

filename = cma11oc(HAx_NAHE);

printf("\nName of file containing observed values: “);

read_name(fi1ename);

printf(“\nTota1 number of observations in file (iterations):

scanf(“Zu”,&nrov);

printf(“\nNumber of leading observations to discard: “);

scanf("Zu",&START);

printf("\nNumber of Lanczos coefficients desired: “);

scanf("%u",&n1anc);

vhi1e(2*n1anc > nrow—START)

{
printf("\nNot enough data for that many coefficients.\n“);
printf("\nEnter a smaller number of Lanczos coefficients: “);
scanf("Xu“,&n1anc);

}

II);

phi = dma11oc(nrow);
cov=dma11oc(2#n1anc);
var=dma11oc(1);
ave=dma11oc(1);

T = dma11oc(n1anc#n1anc); for(i=0;i<n1anc;i++) T[i]=(doub1e)0;

/* observable phi is stored contiguously column-wise (e.g. by HATLAB) t/
mat1abread(phi, nrow, 1,fi1ename);

/* send the observable, offset by START */
moment(phi+START,nrou-START,ave, var, cov,2*n1anc-1);

/* First column of T */

T[0] = a1pha(1);
if((temp=betasq(1))>HACHEPS*10)
{

T[1]=sqrt(temp);

/* General column of T */

J'=2:

for(i=1;i<n1anc-1, f1ag==O;)
{

T[itn1anc+i-1]=sqrt(betasq(i));

48

main()

{

char tfilename;

FILE tofp;

double temp=0, #T;

long i, j, nrow, nlanc, START;
int f1ag=0;

filename = cma11oc(HAx_NAH);

princf("\nName of file containing observed values: “);
read_name(fi1ename);

printf("\nTota1 number of observations in file (iterations): “);
scanf("2u“,&nrow);

printf("\nNumber of leading observations to discard: “);
scanf("Zu",&START);

printf("\nNumber of Lanczos coefficients desired: “);
scanf("%u“,&n1anc);

whi1e(2*n1anc > nrou-START)

{
printf("\nNot enough data for that many coefficients.\n");

printf("\nEnter a smaller number of Lanczos coefficients: “);
scanf("Zu",&n1anc);
}

phi = dma11oc(urow);
cov=dma1loc(2*n1anc);
var=dma11oc(1);
ave=dma11oc(1);

T = dma11oc(n1anc*n1anc); for(i=0;i<n1anc;i++) T[i]=(doub1e)O;

/* observable phi is stored contiguously column-wise (e.g. by HATLAB) */
mat1abread(phi, nrov, 1,fi1ename);

/* send the observable, offset by START #/
moment(phi+START,nrov-START,ave, var, cov,2*n1anc-1);

/# First column of T */

T[0] = a1pha(1);
if((temp=betasq(1))>HACHEPS#10)
{

T[1]=sqrt(temp);

/* General column of T */

J‘=2:

for(i-1;i<n1anc-1, f1ag==0;)
{

T[i*nlanc+i-1]=sqrt(betasq(i));

48


% 49.txt
T[i*n1anc+i] = a1pha(i+1);
if((temp = betasq(i+1))>HACmEPS#10)
{

T[i#n1anc+i+1] = sqrt(temp);

i++;
}
else

f1ag=1;

}

/t Last column */
if(f1ag!=1 it ((temp= betasq(n1anc-1))>HACHEPSt10))

{
T[n1anc#n1anc-2] = sqrt(temp);
T[n1anctn1anc-1] = a1pha(n1anc);
printf("\nbeta(1) = %1f\nbeta(Zd) = Zlf (last beta)",
T[1],(nlanc-1),T[nlanc*n1anc-2]);
}
else
{
printf("\nApproximate invariant space reached at step %d.",i);
printf("\nbeta(1) = Z1f\nbeta(Zd) = Zlf (last accurate beta)“.
T[1],i,T[itn1anc+i—1]);
printf("\nbeta(Zd)“2: Zlf (first spurious resu1t)“,i+1,temp);
}

printf("\nThe matrix T is: \n");
matprint(T,n1anc,n1anc);

}

else

{

printf("\nmain(): Approximate invariant space reached at first step.“);

49

printf("\na1pha(1) - Z1f\nbeta(1)”2: Zlf (first spurious result)“, T[O], temp);

}
ofp = fopen("Tmat.m","v“);
check(ofp);
mat1aburite(T,n1anc,n1anc,ofp);
fc1ose(ofp);
}
void read_name(char tname)
{
int c, i = 0;
while ((c = getchar()) != EOF it c != ’ ’ it c !- ’\n’)
name[i++] = c;
name[i] = ’\0’;
}

/#t* end 1anczos.c *#*/

% 50.txt
50

/llnkttttttttinlntint*1!!!##1##!ttitilltit###t¥#tt¥ttItt####t#ttII=##mlltttt

* functions.c functions required by 1anczos.c t
# t
* Created by Hilliam J. De Meo #
* on 1/7/98 *
t Last modified 1/10/98 :

¥¥##¥$¥##¥¥#$¥¥¥###¥¥$*I#****#**####*######¥#¥#¥#$#¥¥#l¥**#****/

#inc1ude <math.h>

#define START 1000

#define ITER 10000

/t Machine constants on dino (Sun Ultra 1 at Courant) */

#define HACHEPS 1.15828708535533604981e-16

#define SQRTEPS 1.07623746699106147048e-O8

double a1pha(int j);

double betasq(int j);

double form1(int j, int k);

double form2(int j, int k);

double moment(doub1e tdata, long n, double tave, double tvar, double tcov, int k);

/t external variables to be used by functions */
double *phi,tcov, *var, tave;

double alpha(int j)
{

/* alpha is never called with j < 1 */
if(j==1)
return
form1(1,1);
else if(j>1)
return
((doub1e)1/betasq(j-1))*
(form1(j-1,3) — pou(a1pha(j-1),3)
- 2 * (alpha(j-1)#betasq(j-1) + sqrt(betasq(j-2))#form2(j-1,2))
+ betasq(j-2));
}

double betasq(int j)

{
if(j==0)
return (doub1e)O;
else if(j>0)
return (form1(j,2) - pow(a1pha(j),2) - betasq(j-1));
}

double form1(int j, int k)
{

% 51.txt
51

double form14,alphal,alpha1sq,form12,form13;
if(j==0)

return (doub1e)0;
else if(j==1)
{

/# printf("\nvar = %1f, cov(%d) = %1f \n“,*var,k,cov[k]);t/

return (cov[k])/(*var); /* the only real value */
}
else if(j>1)
{

return ((double)1/betasq(j-1))

* ( form1(j-1,k+2)
+ pow(a1pha(j-1),2)*form1(j-1,k)
+ 2t(a1pha(j-1) t sqrt(betasq(j-2)) * form2(j-1,k)
- a1pha(j-1)*form1(j-1,k+1)
- sqrt(betasq(j—2))#form2(j-1,k+1) )
+ betasq(j-2)*form1(j-2,k)):
}
}

double form2(int j, int k)
{
/* form2 is never called with j < 1 */
if(j==1)
return (doub1e)0;
else if(j>1)
return
(pov(betasq(j-1),—.5)) 3
(form1(j-1,k+1) - a1pha(j-1)*form1(j-1,k)
- sqrt(betasq(j-2)) # form2(j—1,k));
}

/* moment() function for computing var and cov(k)
arguments:
data = a nxi array of doubles
n = length of data[]
ave =(on exit)= the average of data[]
var =(on exit)= the variance of data[]
cov =(on exit)= the covariance of data[i] and data[i+j] for i-1,...,k
k = the max lag for cov above
*/
double moment(double tdata, long n, double tave, double tvar, double tcov, int k)
{
/* Centered about data[0] algorithm: #/

long i,j;
double avel, ave2;

% 52.txt
52

*ave=O;tvar=0; ave1=ave2=O;
for(j=0;j<=k;j++)
cov[j]=0;

for(i=1;i<n;i++)
{
rave += (data[i] - data[0]);
tvar += (data[i] - data[0])#(data[i] - data[0]);
}
tvar /= (doub1e)(n-1);
Ivar -= (((*ave)/(doub1e)n) * ‘(*ave)/(doub1e)(n-1)));
/# *ave = ((*ave)/(doub1e)n) + data[0]; <- that’s the true average (not needed)*/

for(j=0;j<=k;j++)
{

for(i=0;i<(n-j);i++)

{

avel += (data[i] - data[0]);
ave2 += (data[i+j] - data[0]);
cov[j] += (data[i] - data[0])*(data[i+j] — data[0]);

}

avel/=(doub1e)(n-j); ave2/=(doub1e)(n-j);

cov[j] = ((cov[j] - (double)(n-j)tave1¢ave2)/(double)(n-j-1));
}

}

/trt end funccions.c ###/


