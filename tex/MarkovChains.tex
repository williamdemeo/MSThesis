\chapter{Markov Chains}

\section{General Theory}

This review of Markov chain theory can be found in any good probability text. The present
discussion is most similar to that of Durrett~\cite{Durret:1996}, to which we
refer the reader desiring greater detail. 

\subsection{The Basic Setup}

Heuristically, a Markov chain is a stochastic process with a lack of memory property. Here
this means that the future of the process, given its past behavior and its present state, depends only
on its present state. This is the probabilistic analogue of a familiar property of classical particle
systems. Given the position and velocities of all particles at time t, the equations of motion can be
completely solved for the future evolution of the system. Thus, information describing the behavior
of the process prior to time t is superﬂuous. To be a bit more precise, if technical, we need the
following definitions.

\begin{definition}
%Definition 2.1.1 
Let $(\S, \S)$ be a measurable space. A sequence $X_n$, $n\geq 0$, of random variables
taking values in \S\ is said to be a Markov chain with respect to the filtration 
$\sigma(X_0, \dots, X_{n})$ if for all $B \in \S$,
\begin{equation}
% Eqn 2.1
\label{eq:1}
\bP(X_{n+1} \in B \mid \sigma(X_0, \dots, X_{n}))
=\bP(X_{n+1} \in B \mid \sigma(X_{n})).
\end{equation}
\end{definition}
Equation~(\ref{eq:1}) merely states that if we know the present location or state of $X_{n}$,
then information about earlier locations or states is irrelevant for predicting $X_{n+1}$.

\begin{definition}
%% Definition 2.1.2 
A function $p : S \times \S \rightarrow \R$ is said to be a \emph{transition probability} if:
\begin{enumerate}
\item for each $x \in S$, $A \mapsto p(x, A)$ is a probability measure on $(S, \S)$.
\item for each $A \in \S$, $x \mapsto p(x, A)$ is a measurable function.
\end{enumerate}
\end{definition}
We call $X_n$ a Markov chain with transition probabilities $p_n$ if
\begin{equation}
% Eqn 2.2
\label{eq:2.2}
\bP(X_{n+1} \in B \mid  \sigma(X_n)) = p_n(X_n, B)
\end{equation}
The spaces $(S, \S)$ that we encounter below are standard Borel spaces, so the existence of the
transition probabilities follows from the existence of regular conditional
probabilities on Borel spaces---a standard measure theory result 
(see e.g.\cite{Durret:1996}). %[3] page 230

Suppose we are given an initial probability distribution $\mu$ on $(S, \S)$ and a sequence $p_n$ of
transition probabilities. We can define a consistent set of finite dimensional distributions by
\begin{equation}
% Eqn 2.3
\label{eq:2.3}
\bP(X_j\in B_j ,0 \leq j \leq n) = \int_{B_0} \mu(dx_0) \int_{B_1} p_0(x_0, dx_1) 
\int_{B_2} p_1(x_1, dx_2) \cdots 
\int_{B_n} p_{n-1}(x_{n-1}, dx_n).
\end{equation}
Furthermore, denote our probability space by
\[
(\Omega, \F) = (S^\omega,\S^\omega), \quad \text{where} \omega = \{0,l,...\}.
\]
We call this \emph{sequence space} and it is defined more explicitly by
\[
S^\omega  = \{(\omega_0, \omega_1, \dots) : \omega_i in S\}\quad \text{ and }
\quad
\S^\omega  = \sigma(\omega : \omega_i \in A_i \in \S).
\]
% 07.txt
The Markov chain that we will study on this space is simply $X_n(\omega) = \omega$, the coordinate maps.
Then, by the Kolmogorov extension theorem, there exists a unique probability measure $\bP_\mu$ on
$(\Omega, \F)$ so that the $X_n(\omega)$ have finite dimensional distributions~(\ref{eq:2.3}).

If instead of $\mu$, we begin with the initial distribution $\delta_x$, i.e., point mass at $x$, then we
denote the probability measure by $\bP_x$. With such measures defined for each $x$, we can in turn
define distributions $\bP_\mu$, given any initial distribution $\mu$, by
\[
\bP_\mu(A) = \int \mu(dx)\bP_x(A).
\]

That the foregoing construction---which, recall, was derived merely from an
initial distribution $\mu$ and a sequence $p_n$ of transition
probabilities---satisfies Definition~(\ref{eq:2.2}) of a Markov chain is not
obvious, and a proof can be found in~\cite{Durret:1996}.

To state the converse of the foregoing, if $X_n$ is a Markov chain with
transition probabilities $p_n$ and initial distribution $\mu$, then its finite
dimensional distributions are given by~(\ref{eq:2.3}). Proof of this is also found
in~\cite{Durret:1996}. 

Now that we have put the theory on a firm, if abstract, foundation, we can bring
the discussion down to earth by making the forgoing a little more
concrete. First, we specialize our study of Markov chains by assuming that our
chain is \emph{temporally homogeneous}, which means that the transition
probabilities do not depend on time; i.e., 
$p_n(\omega_n, B) = p(\omega_n, B)$. (This is the stochastic analogue of a
conservative system.) 
Next we assume that our state space $S$ is finite, and suppose for all states
$i,j \in S$ that $p(i, j) \geq 0$, and $\sum_j p(i, j) = 1$ for all $i$. In
this case, equation~(\ref{eq:2.2}) takes a more intuitive form,
\[
\bP(X_{n+1}=j \mid X_n = i) =p(i,j),
\]
and our transition probabilities become
\[
p(i,A) = \sigma_{j\in A} p(i,j).
\]
% 08.txt
If $\P$ is a matrix whose $(i, j)$ element is the transition probability 
$p(i, j)$ then P is a \emph{stochastic matrix}; that is, a matrix with elements
$p_{ij}$ satisfying 
\[
\pij \geq 0, \quad \sum_j \pij =1, \quad (i, j =1, 2, \dots, d).
\]
We also refer to $\P$ as the transition probability matrix.

Without loss of generality, we can further suppose our Markov chain is \emph{irreducible}. This
means that, for any states $i, j$, starting in state $i$ the chain will make a
transition to state $j$ at some future time with positive probability. This
state of affairs is often described by saying that all states \emph{communicate}. We
lose no generality with this assumption because any \emph{reducible} Markov chain can
be factored into irreducible classes of states which can each be studied
separately. 

The final two conditions we place on the Markov chains considered below will cost us
some generality. Nonetheless, there remain many examples of chains meeting these conditions
and making the present study worthwhile. Furthermore, it may be the case that, with a little
more work, we will be able to drop these conditions in future studies. The first condition is that
the chain is \emph{aperiodic}. If we let $I_x = \{n \geq 1: p^n(x,x) > 0\}$, we
call a Markov chain \emph{aperiodic} if, for any state $x$, the greatest common
divisor of $I_x$ is 1. The second assumption is that our chain is 
\emph{reversible}. This characterization is understood in terms of the following definition.

\begin{definition}
%% Definition 2.1.3 
\label{def:2.1.3}
A measure $\mu$ is called \emph{reversible} if it satisfies
\[
\mu(x)p(x,y) = \mu(y)p(y,x), \quad  \text{ for all $x$ and $y$.}
\]
We call a Markov chain \emph{reversible} if its stationary distribution (defined
in Section~\ref{sec:convergence-theorem}) is reversible. 
\end{definition}



% ---------- 09.txt ------------------------------------------------------------

\subsection{A Convergence Theorem}
% Section 2.1.2
\label{sec:convergence-theorem}
In succeeding arguments, we use some results concerning the asymptotic behavior of
Markov chains. These results require a few more definitions.

\begin{definition}
%% Definition 2.1.4
A measure $\pi$ is said to be a \emph{stationary measure} if
\begin{equation}
% Eqn 2.4
  \label{eq:2.4}
\sum_x \pi(x) p(x,y) = \pi(x).
\end{equation}
\end{definition}

Equation~(\ref{eq:2.4}) says $\bP_\pi(X_1 = y) = \pi(y)$, 
and by induction that $\bP_\pi(X_n = y) = \pi(y)$ for all $n \geq 1$. 
If $\pi$ is a probability measure, then we call $\pi$ a stationary distribution. It
represents an equilibrium for the chain in the sense that, if $X_0$ has
distribution $\pi$, then so does $X_n$ for all $n$.

When the Markov chain is irreducible and aperiodic, the distribution of the state at time
$n$ converges pointwise to $\pi$ as $n \rightarrow \infty$, regardless of the
initial state. It is convenient to state this convergence result in terms of the
Markov chain's transition probability matrix $\P$. Before doing so, we note that
irreducibility of a Markov chain is equivalent to irreducibility (in the usual 
matrix theory sense) of its transition probability matrix. Furthermore, it turns
out that a transition probability matrix of an aperiodic Markov chain falls into
that class of matrices often called acyclic, but for simplicity we will call
such stochastic matrices aperiodic. With this terminology, we can state the
convergence theorem in terms of the transition probability matrix \P. 

% Theorem 2.1.1
\begin{theorem}
\label{thm-2.1.1}
Suppose $\P$ is irreducible, aperiodic, and has stationary distribution
$\pi$. Then as $n\rightarrow \infty$, $p^n(i,j) \rightarrow \pi(j)$.
\end{theorem}

The notation $p^n(i,j)$ means the $(i,j)$ element of the $n$th power of $\P$.

A Markov chain whose transition probability matrix satisfies the hypotheses of
Theorem~\ref{thm-2.1.1} is called \emph{ergodic}. If we simulate an ergodic chain for
sufficiently many steps, having 
%
%
%
% ---------------10.txt --------------------------------------------------
%
%
%
begun in any initial state, the final state is a sample point from a
distribution that is close to $\pi$. 

To make this statement more precise requires that we define ``close.''
\begin{definition}
%% Definition 2.1.5 
Let $\pi$ be a probability measure on $S$. Then the \emph{total variation distance} at time
$n$ with initial state $x$ is given by 
\[
\Delta_x(n) = \|\P^n(x, A) - \pi(A)\|_{TV} = \max_{A\in \S} | \P^n(x,A)-\pi(A)|.
\]
\end{definition}
In what follows, we will measure rate of convergence using the function
$\tau_x(\epsilon)$, defined as the first time after which the total variation
distance is always less than $\epsilon$. That is, 
\[
\tau_x(\epsilon) = \min\{m : \Delta_x(n) \leq \epsilon \text{ for all } n\geq
m\}.
\]

To begin our consideration of the connection between convergence rates of Markov
chains and eigenvalues, we first note that an aperiodic stochastic matrix $\P$ has real
eigenvalues 
$1 = \lambda_0 > \lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_{d-1} \geq -1$, 
where $d = |S|$ is the dimension of the state space. For an ergodic chain, the
rate of convergence to the stationary distribution $\pi$ is bounded by a
function of the \emph{subdominant} eigenvalue. By subdominant eigenvalue we mean that
eigenvalue which is second largest in absolute value, and we denote this
eigenvalue by $\lambda_{\max} = \max{\lambda_1, |\lambda_{d-1}|}$. 
The function bounding the rate of convergence of a Markov chain is given by the
following theorem ($\log$ denotes the natural logarithm):
\begin{theorem}
% Theorem 2.1.2
\label{thm-2.1.2}
The quantity $\tau_x(\epsilon)$ satisfies
\begin{enumerate}
\item 
$\tau_x(\epsilon) \leq (1-\lambda_{\max})^{-1}(\log \pi(x)^{-1} + \log \epsilon^{-1})$;
\item $\max_{x \in S} \tau_x(\epsilon) \geq \frac{1}{2} \lambda_{\max}(1-\lambda_{\max})^{-1} \log(2\epsilon)^{-1}$.
\end{enumerate}
\end{theorem}
As this theorem shows, if we have an upper bound on the subdominant eigenvalue, then we have
an upper bound on the function $\tau_x(\epsilon)$. In what follows, we will
derive an approximation to the 
% 11.txt
subdominant eigenvalue and supply error bounds. Together, an approximation and error bounds
for $\lambda_{\max}$  provide enough information to make Theorem~\ref{thm-2.1.2} useful.

\section{Functions on the State Space}
% Sec 2.2
\label{sec:funct-state-space}
Recall that $X_n(\omega) = \omega_n\in S$ denotes the state in which the Markov
chain exists at time $n$. 
Suppose that 
$\Phi = \{\phi_1, \dots, \phi_p\}$ 
is a collection of $p$ \emph{observables}, or functions defined on the state
space $S$. Furthermore, let these observables be real valued, 
$\phi_i : S \rightarrow \R$. 
It is often useful to assume that none of the observables is a constant function. Suppose now that
the state space $S$ is finite with $d$ possible states. Then, since an
observable is simply a map of the state space, we can think of each $\phi_i$ as a
vector of $d$ real numbers---the $d$ values that it takes on at the different states.

Now assume the Markov chain is irreducible, and let $\pi$ denote its stationary distribution.
If we start the chain from its stationary distribution---i.e., suppose $X_0$ has
distribution $\pi$---then $X_n$ is a stationary process. 
Furthermore, for each $i$, $\phi_i(X_n)$ is a stationary stochastic process with
\emph{mean}
\[
\bE_\pi\phi_i = \sum_{x\in S} \pi(x)\phi_i(x)
\]
and \emph{autocovariance function}
\begin{align}
% Eqn 2.5
\bC_\pi (\phi_i(X_n),  \phi_i (X_{n+s})) 
&= \bE_\pi[(\phi_i(X_n) - \bE_\pi\phi_i)(\phi_i(X_{n+s}) - \bE_\pi\phi_i)]\\
&= \sum_{x, y\in S} \bP_\pi(X_n = x, X_{n+s} = y) (\phi_i(x)- \bE_\pi\phi_i)(\phi_i(y) - \bE_\pi\phi_i).\nonumber
\end{align}
By the definition of conditional probability, we can write~(\ref{eq:2.4}) as follows:
\[
\sum_{x, y\in S} \bP_\pi(X_n = x) \bP_\pi(X_{n+s} = y\mid X_n = x) (\phi_i(x)-
\bE_\pi\phi_i)(\phi_i(y) - \bE_\pi\phi_i).
\]
Equivalently,
\[
\sum_{x, y\in S} \pi(x)p^s_{xy} (\phi_i(x)-\bE_\pi\phi_i)(\phi_i(y) - \bE_\pi\phi_i).
\]
Here $p^s_{xy}$ denotes the element in row $x$ and column $y$ of $\P^s$, the
$s$th power of the transition probability matrix. 
Similarly, we define the \emph{cross-covariance} between the function 
$\phi_i$ at time $n$ and $\phi_j$ at time $n+s$ as
\begin{align}
% Eqn 2.6
\bC_\pi (\phi_i(X_n),  \phi_j (X_{n+s})) 
&= \bE_\pi[(\phi_i(X_n) - \bE_\pi\phi_i)(\phi_j(X_{n+s}) - \bE_\pi\phi_j)]\nonumber\\
&= \sum_{x, y\in S} \pi(x)p^s_{xy} (\phi_i(x)-\bE_\pi\phi_i)(\phi_j(y) - \bE_\pi\phi_j).
\end{align}
Now let $\<\Phi\>$ denote the matrix of mean vectors whose $j$th column is 
$\bE_\pi\phi_j\one$, where $\one = (1,\dots, 1)^t$,
and let $\Pi = \diag(\pi(\omega_1),\dots, \pi(\omega_d))$ be the $d \times d$
diagonal matrix with stationary probabilities $\pi(\omega)$
on the main diagonal and zeros elsewhere. 
Finally, denoting by $\C(s)$ the $p \times p$ covariance matrix
whose $(i,j)$ element is $\bC_\pi(\phi_i(X_n), \phi_j(X_{n+s}))$, we have
\begin{align*}
\C(0) &= \E(\Phi(X_n) - \<\Phi\>)(\Phi(X_n) - \<\Phi\>)^t\\
&= (\Phi - \<\Phi\>)^t\Pi (\Phi-\<\Phi\>), \\
\C(s) &= \E(\Phi(X_n) - \<\Phi\>)(\Phi(X_{n+s}) - \<\Phi\>)^t\\
&= (\Phi - \<\Phi\>)^t \Pi \P^s (\Phi - \<\Phi\>).
\end{align*}

Below, we will also find it useful to have at our disposal a new matrix that is \emph{similar} to the
transition probability matrix. We have in mind the matrix 
$\M = \Pi^{1/2}\P\Pi^{-1/2}$. 
As is easily verified, this allows us to write the covariance matrix as
\begin{align}
\label{eq:2.7}
\C(s) &= 
(\Phi - \<\Phi\>)^t \Pi^{\frac{1}{2}t}\M^s\Pi^{\frac{1}{2}}(\Phi - \<\Phi\>)\nonumber\\
&=\Psi^t \M^s \Psi,  % Eqn 2.7
\end{align}
%% \\Phi\Phi‘M’\\Phi\Phi (2.7)
where we have defined $\Psi = \Pi^{\frac{1}{2}}(\Phi - \<\Phi\>)$. Recall that
our main motivation is that for out-of-core problems traditional eigenvalue
algorithms are inadequate. With this fact and the form~(\ref{eq:2.7}) % (2.7) 
in 
%
%
% ------------------ 13.txt ---------------------------------------------------
%
%
mind, we will consider using the covariance of observables on the state space to implement the
%% (otherwise useless) 
Rayleigh-Ritz procedure, which we describe below. This procedure requires
that $\M$ be symmetric. As the next fact demonstrates, 
%% this required symmetry is the reason for our
%% interest in the reversibility property of the Markov chain.
this need for symmetry is the reason we insist that the Markov chain be reversible.
\begin{fact}
%% Fact 2.2.1 
The matrix $\M$ is symmetric if and only if the Markov chain is reversible (i.e., iff the
process satisfies the \emph{detailed balance} condition).
\end{fact}
\begin{proof}
  \begin{align*}
  \M \text{ is symmetric } 
& \quad \Longleftrightarrow  \quad
 (\Pi^{1/2}\P\Pi^{-1/2})^t = \Pi^{1/2}\P\Pi^{-1/2}\\
& \quad \Longleftrightarrow  \quad
 \Pi^{-\frac{1}{2}t}\P^t\Pi^{\frac{1}{2}t} = \Pi^{1/2}\P\Pi^{-1/2}\\
& \quad \Longleftrightarrow  \quad
 \P^t\Pi^t = \Pi\P.
  \end{align*}
Elementwise, the final equality is $\pi_i\pij = \pi_j\pji$. 
According to Definition~\ref{def:2.1.3}, this states
that $p$ is a reversible measure.
\end{proof}

% 14.txt

