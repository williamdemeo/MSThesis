\begin{center}
\newcommand\skipsize{6pt}
A Lanczos Procedure for Approximating Eigenvalues of Large Stochastic Matrices\\[\skipsize]
by\\[\skipsize]
William J. DeMeo\\[\skipsize]
Master of Science in Mathematics\\[\skipsize]
New York University\\[\skipsize]
Professor Jonathan Goodman, Chair
\end{center}

The rate at which a Markov chain converges to a given probability distribution
has long been an active area of research. Well known bounds on this rate of
convergence involve the subdominant eigenvalue of the chain's underlying
transition probability matrix. However, many transition probability matrices are
so large that we are unable to store even a vector of the matrix in fast
computer memory. Thus, traditional methods for approximating eigenvalues are
rendered useless. 

In this paper we demonstrate that, if the Markov chain is reversible, and we
understand the structure of the chain, we can derive the coefficients of the
traditional Lanczos algorithm without storing a single vector. We do this by
considering the variational properties of observables on the chain's state
space. In the process we present the classical theory which relates the
information contained in the Lanczos coefficients to the eigenvalues of the
Markov chain. 
